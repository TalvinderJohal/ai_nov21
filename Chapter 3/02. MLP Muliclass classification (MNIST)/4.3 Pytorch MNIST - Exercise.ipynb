{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\">Deep Learning Fundamentals</h2>\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">Multiclass Clasification: MNIST</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxliary plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://discuss.pytorch.org/t/view-classify-in-module-helper/30279/6\n",
    "\n",
    "def view_classify(img, ps):\n",
    "\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST Dataset\n",
    "First up, we need to get our dataset. This is provided through the `torchvision` package. The code below will download the MNIST dataset, then create training and test datasets for us. Don't worry too much about the details here, you'll learn more about this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data (Preprocessing)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5), (0.5)) ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset    = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset    = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into `trainloader` and we make that an iterator with `iter(trainloader)`. We'd use this to loop through the dataset for training, but here I'm just grabbing the first batch so we can check out the data. We can see below that `images` is just a tensor with size (64, 1, 28, 28). So, 64 images per batch, 1 color channel, and 28x28 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch\n",
    "\n",
    "Here I'll use PyTorch to build a simple feedfoward network to classify the MNIST images. That is, the network will receive a digit image as input and predict the digit in the image.\n",
    "\n",
    "<img src=\"assets/mlp_mnist.png\" width=600px>\n",
    "\n",
    "To build a neural network with PyTorch, you use the `torch.nn` module. The network itself is a class inheriting from `torch.nn.Module`. You define each of the operations separately, like `nn.Linear(784, 128)` for a fully connected linear layer with 784 inputs and 128 units.\n",
    "\n",
    "The class needs to include a `forward` method that implements the forward pass through the network. In this method, you pass some input tensor `x` through each of the operations you defined earlier. The `torch.nn` module also has functional equivalents for things like ReLUs in `torch.nn.functional`. This module is usually imported as `F`. Then to use a ReLU activation on some layer (which is just a tensor), you'd do `F.relu(x)`. Below are a few different commonly used activation functions.\n",
    "\n",
    "<img src=\"assets/activation.png\" width=700px>\n",
    "\n",
    "So, for this network, I'll build it with three fully connected layers, then a softmax output for predicting classes. The softmax function is similar to the sigmoid in that it squashes inputs between 0 and 1, but it's also normalized so that all the values sum to one like a proper probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    # Defining the layers, 128, 64, 10 units each\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    # Forward pass through the network, returns the output logits\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential API\n",
    "PyTorch provides a convenient way to build networks like this where a tensor is passed sequentially through operations, `nn.Sequential` ([documentation](https://pytorch.org/docs/master/nn.html#torch.nn.Sequential)). Using this to build the equivalent network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (5): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size   = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in an `OrderedDict` to name the individual layers and operations. Note that a dictionary keys must be unique, so _each operation must have a different name_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1',   nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2',   nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('output', nn.Linear(hidden_sizes[1], output_size)),\n",
    "          ('softmax', nn.Softmax(dim=1))]))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights and biases\n",
    "\n",
    "The weights and such are automatically initialized for you, but it's possible to customize how they are initialized. The weights and biases are tensors attached to the layer you defined, you can get them with `model.fc1.weight` for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0198, -0.0343,  0.0275,  ...,  0.0076, -0.0247, -0.0214],\n",
      "        [-0.0236, -0.0104, -0.0237,  ..., -0.0134,  0.0007,  0.0339],\n",
      "        [-0.0072, -0.0026, -0.0290,  ..., -0.0335,  0.0316,  0.0220],\n",
      "        ...,\n",
      "        [ 0.0177,  0.0280,  0.0093,  ...,  0.0229,  0.0131,  0.0219],\n",
      "        [-0.0004, -0.0303,  0.0215,  ..., -0.0063,  0.0191, -0.0225],\n",
      "        [-0.0239,  0.0028, -0.0223,  ..., -0.0099, -0.0155,  0.0276]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-2.4711e-02, -1.7415e-02, -3.1192e-02, -2.9636e-02,  2.8431e-02,\n",
      "         1.6659e-02, -9.0070e-03, -1.2588e-03, -2.4047e-02, -8.0797e-03,\n",
      "         2.2847e-02, -2.8866e-02, -7.4551e-03, -2.9822e-02,  2.9739e-02,\n",
      "         2.8335e-02,  2.6870e-02, -4.0727e-03,  6.8940e-03, -1.5600e-02,\n",
      "        -1.0925e-02,  8.6389e-03,  6.3622e-03,  2.0063e-02, -3.0288e-02,\n",
      "        -3.3173e-02, -1.9943e-02,  3.2794e-02, -2.7190e-02,  1.3579e-02,\n",
      "        -1.3802e-02,  8.8254e-03, -2.4460e-02, -2.6944e-02,  2.6313e-02,\n",
      "        -2.9234e-02, -3.2553e-02,  1.4716e-02, -3.5331e-02,  3.9446e-05,\n",
      "         1.3907e-02, -1.1381e-02, -1.7045e-02,  3.2781e-02, -3.0316e-02,\n",
      "         2.4581e-02, -5.7099e-03, -3.2669e-02, -2.7373e-02,  3.9507e-03,\n",
      "         9.8606e-03, -2.1782e-02, -3.1604e-02,  2.5428e-02,  5.4151e-03,\n",
      "         1.7517e-02, -9.7564e-03,  1.6454e-02, -1.7954e-02, -6.6863e-03,\n",
      "         3.2279e-02, -1.4773e-03, -2.1672e-02, -1.3162e-02, -1.2973e-02,\n",
      "         8.7692e-03,  2.7631e-02, -3.2933e-02,  3.4040e-02, -9.3363e-03,\n",
      "        -3.1391e-02, -1.7715e-02,  1.7952e-02, -3.1106e-03,  4.5076e-03,\n",
      "        -3.0954e-02, -1.4612e-02, -6.9455e-03, -2.6051e-02,  1.4074e-02,\n",
      "         1.6308e-02,  3.3063e-02,  2.0307e-02, -2.3639e-02, -3.3293e-02,\n",
      "         1.8356e-02, -2.4525e-02, -2.8860e-02, -1.6997e-02, -3.1068e-02,\n",
      "         2.7571e-02, -1.0446e-02,  1.0879e-02, -1.1873e-02, -9.4982e-03,\n",
      "         2.7143e-02, -2.1782e-02, -2.5749e-02, -3.5161e-02,  3.0222e-03,\n",
      "        -2.6155e-02,  2.0083e-02, -3.0036e-02,  2.1306e-02,  2.5034e-02,\n",
      "        -2.5959e-02, -1.2539e-02, -2.1510e-02, -2.9002e-02,  2.4761e-02,\n",
      "        -3.5608e-02,  3.0671e-02,  2.3349e-02,  3.2720e-02, -1.9644e-02,\n",
      "        -3.0958e-02, -3.8119e-03,  1.7450e-02,  2.1150e-02,  2.5665e-02,\n",
      "         2.8922e-02,  7.6948e-03,  2.9675e-03,  1.6176e-02, -3.9171e-03,\n",
      "        -1.2834e-02,  2.6968e-02, -1.9309e-02], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)\n",
    "print(model.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For custom initialization, we want to modify these tensors in place. These are actually autograd *Variables*, so we need to get back the actual tensors with `model.fc1.weight.data`. Once we have the tensors, we can fill them with zeros (for biases) or random normal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set biases to all zeros\n",
    "model.fc1.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0061,  0.0007, -0.0131,  ...,  0.0088,  0.0023,  0.0060],\n",
       "        [ 0.0002, -0.0034, -0.0017,  ...,  0.0074,  0.0159,  0.0123],\n",
       "        [-0.0092,  0.0199,  0.0124,  ...,  0.0025, -0.0031, -0.0096],\n",
       "        ...,\n",
       "        [ 0.0015, -0.0102, -0.0162,  ...,  0.0171,  0.0145, -0.0044],\n",
       "        [-0.0012,  0.0028, -0.0241,  ...,  0.0050, -0.0027, -0.0146],\n",
       "        [ 0.0026, -0.0014, -0.0245,  ...,  0.0009, -0.0212, -0.0072]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from random normal with standard dev = 0.01\n",
    "model.fc1.weight.data.normal_(std=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Forward pass\n",
    "\n",
    "Now that we have a network, let's see what happens when we pass in an image. This is called the forward pass. We're going to convert the image data into a tensor, then pass it through the operations defined by the network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAroAAAGHCAYAAABf8fH3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAABYlAAAWJQFJUiTwAAAp30lEQVR4nO3deZgdZZn38e8NCIQtgAgoCgEUAuJCoiCrLMqoUQwgjDMD4y46vKIoMzAqCqPMxBkXUGdERQTFURQFR0ABBQRFxAmLE0EWoRWQfQkBwpbc7x9VbQ7NOZ2Tzumupb+f66qrcqqeqrpP9Un3r59+qioyE0mSJKltVqi6AEmSJGk8GHQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSQIiIstpWtW1TAYRMVSe792actyIOLrc9uR+9xsRu5XLh8ZWsZaHQVeS1CoRsVpEvDcifhQRf4qIRyLi4Yi4OSJOj4gDI2JK1XVOlI4A1jktioh7I+KSiDgsIlarus7JKCJml+F5t6praauVqi5AkqRBiYg3AF8BNuxY/DCwGJhWTvsBn4qIgzLzgomusUIPAw+V/14ZWBfYuZzeGRG7Z+ZdVRXXEPcA1wG3L8M2j5Tb3NZl3WzgLeW/L1qewtSdPbqSpFaIiLcCZ1KE3OuAg4D1MnONzFwLWBt4E0WgeA6waxV1VujTmblhOa0LrAccCySwNcUvCBpFZn4xM6dn5j8vwzaXl9vsOZ61qTuDriSp8SLixcAJFD/XzgG2zcxTM/Pe4TaZOT8zv5+ZuwN/DSyoptp6yMx7M/OjwNfLRW+MiOdUWZM0aAZdSVIbHAusQvHn4b/NzIWjNc7M7wKf7WfHEbFiROweEcdHxNyIuDMiHo+IP0fEGRGxxyjbrhARb42IC8sxsU9ExN0R8buIOCkiXtNlm00j4ksRcX1ELCzHGP8xIi6KiH+OiPX6qXsZfLvj3zM66vjLxXkRsVVEnBIRt5Tv4cwRNW8bEaeW6x+LiHsi4tyI2K+fAiJi44g4sdz+0XI89acjYmqP9itHxKyI+GpEXF0e79HyPH0rImaO03F7Xow2yjGedjHa8DKWDFv4+Mhx1GW7j5Wv/3cpx3hb2e6WiDDbdXCMriSp0SJiI2BW+fLzmTm/n+0yM/s8xFZA51jex4DHgWdTjLGcHREfycx/7bLtN4G/7Xg9H1iLYtjA1uX0k+GVETGDYmjFmuWiJyjG1m5cTq8EruzcZgA6x46u1WX9LhS95atR9II/2bkyIt4NfIklnWcPUAwT2QvYKyJOBd6amYt6HP/5wHeBZ1GMIU6KsdQfouhl3jUzR46J3Qv4UcfrR8rtNqY43wdExNsz85s9jjnW4w7K48CdwFRgVZ46frrTScDHgZkR8aLM/L8e+3t7OT8lMxcPutgmM/VLkppuNyDKf//POOz/ceB7wBsoxv9Oycw1gA2Ao4BFwCcjYvvOjSJiV4rQtRg4DFgrM9emCDbPAd4K/GLEsT5NEXJ/DczIzJUzcx1gdeDlwHEUYXmQNu749wNd1v8X8BvgReVY59UowiARsSNLQu7pwPPKetcGPkIRHg8ERhvT+mmK97RLZq5J8V5nU1z49XzglC7bPEQx5GJPinHYq2fmFGATinO0EvCViNi4y7bLc9yByMxLM3ND4LThWjrGT29YriMzbwXOLdu8rdu+IuL5FBcUJkuGoahk0JUkNd1W5fwxiovQBiozr8/MAzLzrMy8c7gnODPvysxPAsdQBO33jNj0FeX8vMw8LjMXlNtlZt6emadk5uE9tnl/Zl7ZUcMjmfm/mXlYZv5qwG/xXcOHoQi0I90FvDYz53XU/4dy3ScossQvgTeXwYzMfKjs4Z5TtjsiIrr1FkMx5OS1mfmLctvFmflD4IBy/asjYufODTLzosx8e2ZeMGIc9p8y8zCKntBV6REOx3rciny1nB8YEc/osn64N/fijq+LSgZdSVLTPbOc378MwxEGafhP6DuNWP5gOV9/GcZNDm/z7OWuahTlGNetI+JEitutAXwnM+/u0vyL3cY8R8S6wO7ly3/rMTThU8CjwBrA63qU893MvHHkwsy8ELi0fPmm3u+mq15fk/E+7nj4EcUwh2cBr+9cUX6u/r58edIE19UIBl1JkpYiIqZE8WCFiyLirvKCrOGLhoZ7XkfeseCnFMMeZgAXRfGgiqXd1eCccv6NiJgTEa/o0Ys3Fh/vqPkx4HfAO8p1lwH/0GO7Xj3I21L0ZCfw824NyvHSc8uXM7q1YfT7xw7v92nbRsS6EXFURFxaXuj3ZMf7O6NsNtr5HtNxJ1pmPsmSYRQje6j/CtiI4hek0yeyrqbwYjRJUtMN/+l6nYiIQffqRsSzKULRFh2LHwbupxh/uyLFxWWrd26XmTdGxHuBL1Jc0LVLub8hiovJvtI5PKH0j8CWwI7AEeX0aET8imKc8MlLu6PEKDoveFpEMT71WopQ+J0yUHXTrZcXih5GgPmZ2e1CqmG3jmg/UrcHKYxc95RtI2JrigsEN+hYvABYSBG8VwaGxzYvbd99H7dCJwL/BLw2IjbIzDvL5cPDFr6TmY9UU1q92aMrSWq6a8v5KhQhcdCOowi5N1H8mX/d8iEU65cXDb2i14aZeRKwKfAB4IcUoXwaxXjeuRHx4RHt76W4sOjVwOcpeotXphgi8F/AvIh47hjfR+cFTxtl5taZuV95v+FeIReKUDyaVcZYTz+ix/KvU4TcK4DXAGtm5lqZuUH5Ndl/KduP9biVyMwbKHqZV6J4EMrw0JG9yyYOW+jBoCtJarqfU/TiwZIf/AMRESsDbyxf/l1m/iAz7x/RbANGUV7AdnxmzqboIdyOohc1gE9E8bCLzvaZmT/NzPdn5gyK3uKDgfuAzYDPLe/7GpDhnt4pETFaz+dwMO/VMzza8ILhscp/2ba8k8J2FAF878w8t0uP8qhfk7EctwZOLOfDwxcOpPgl6JrM/HU1JdWfQVeS1Gjllf7DY1vfN8rV/U8REf302q3Hkh7LkcMMhr2qn+PBX0Lsbyh6HG+l+Dk86pX9mXl/Zn4FGO79fWW/xxtnV7LkF4zduzUoH7ww/PCGK3rsZ7T3M7yuc9u/BOfM7DX8oJ+vybIedzwM3/O2n8/i6RS3f9u6vJXdcOC1N3cUBl1JUht8lOICq+cC/x0Rq47WOCIOAD7Yx34fZEmYe1GX/TwbeF+PY6zca6flHQqeKF+uUrZfISJGu3ZmYWf7qmXmfcCF5csjetxZ4giK23w9xJJfRkb664jYbOTC8j7Ew3dN+F7HquH7CG8QEet32e5FPPUhHb0s63HHw/BdNtZeWsPMfBQ4tXz5GeClFJ+h0R6KMekZdCVJjZeZVwGHUITSWcCV5V0O1h1uExFTI2LfiLiQ4kb9a3bd2VP3+xDFHQkAToqIl5b7WiEi9qQYNtGrN+5fI+L0iJg9oo4NIuLzFGN3Ezi/XLUWcGNEfCQiXhQRK4441rFlu3Opj6MoeiVnAN8ZHj8cEWuU44+PLNvNycwHe+zjceDH5cMnht/vG1hyF4HzM/OXHe2vpegND+C08oEJRMQzImJfivM52sVxYz3uePhdOX9N+UvT0gzfU3c4iJ+VmXcNvqwWyUwnJycnJ6dWTBRPtrqTIkAOTwtY0jM7PA0Bu47YdnjdtBHLt2fJI2aTIkQNv76XYgxvUj5VuGO740Ycc36XOj7c0X7tEeseL/f/ZMeyPwDPXcZzMlRue/Qybtf1fHRpdzDFeNmkCL33jaj5VGDFUep6J8VDKYa/Vp3n+gbg2V223afjmFme18fKf/+RYvxqAkMDPu7R5fqTR9nvbiOW7zZKLeuVX+Ms38/t5X6e1rZjm9901Pn6qv/P1X2yR1eS1BqZeSbFBVuHUPyp/FaKK9VXoggQp1P8WXvLzLy4z33+GtgBOJPilmLPoAhIX6b48/HVPTb9HHAoxd0WrqfogVwFuIWiR3nXLJ4eNuxBigcCHAdcTnEh1JoUtwX7DcUjdV+a5dPH6iIzv0zxeOL/pghqa1CE+vOB/TPzwOz+MIlhNwIvoxhrOp/idm1DFH+ef1lm3t7lmGcAe5THWEDxNfkjxWN9t2XJLc1Gs8zHHbTMvIdifPMPKL7ez6J4jPEmo2z2g3J+O/DjcS2wBaL87UCSJEk1FxHnU1xs96nMPHJp7Sc7g64kSVIDlOORry9fbpFdHmGsp3LogiRJUs1FxBrAFyiGwJxlyO2PPbqSJEk1FREfoHiy3oYUY7wfBWZm5jUVltUY9uhKkiTV19oUF6ctAi4F9jLk9s8eXUmSJLWSPbqSJElqJYOuJEmSWsmgK0mSpFZaaawbvnqF/R3cK6mxzl/8vai6BknS+LJHV5IkSa005h5dSVJzRMTNwFrAUMWlSNKymgY8mJmbLuuGBl1JmhzWmjJlyrpbbbXVulUXIknL4tprr2XhwoVj2tagK0mTw9BWW2217ty5c6uuQ5KWycyZM7niiiuGxrKtY3QlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrrVR1AZKkiTHvtvlMO/Lscdv/0JxZ47ZvSRoLe3QlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlqQai8PaIuCwiFkTEIxFxZUQcGhErVl2fJDWRQVeS6uEU4GvApsBpwFeBlYHjgdMiIiqsTZIayduLSVLFImI2cBBwM7BdZt5TLn8G8F1gP+AtwMkVlShJjWSPriRVb99y/pnhkAuQmU8AR5Uv3zfhVUlSwxl0Jal6G5bzm7qsG142IyLWnphyJKkdHLogSdUb7sXdtMu6zTr+PR24bLQdRcTcHqumj6EuSWo0e3QlqXpnlfMPRsS6wwsjYiXgmI5260xoVZLUcPboSlL1vgMcCLwWuCYi/gd4BHgVsDlwA/ACYNHSdpSZM7stL3t6ZwyqYElqAnt0JalimbkY2Bs4HLiD4g4MbwduBXYG7i2b3lVJgZLUUPboSlINZOaTwGfK6S8iYgrwUmAh8LuJr0ySmsseXUmqt4OAVYHvlrcbkyT1yaArSTUQEWt1WfZyYA7wEPAvE16UJDWcQxckqR7Oj4iFwDxgAfBC4HXAY8C+mdntHruSpFEYdCWpHk4H3kxx94UpwJ+BE4E5mTlUYV2S1FgGXUmqgcz8D+A/qq5DktrEMbqSJElqJYOuJEmSWsmhC5I0SWyz0VTmzplVdRmSNGHs0ZUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIredcFSZok5t02n2lHnj0hxxry7g6SasAeXUmSJLWSQVeSJEmtZNCVJElSKxl0JakmImJWRJwXEbdGxMKIuCkivhcRO1RdmyQ1kUFXkmogIj4FnAXMAH4CHA9cAbwR+GVEHFhheZLUSN51QZIqFhEbAocDdwIvzsy7OtbtDlwA/AtwajUVSlIz2aMrSdXbhOL78a87Qy5AZl4ILACeVUVhktRkBl1Jqt4NwOPAdhGxXueKiNgVWBP4aRWFSVKTOXRBy23FDdbvu+16Zz7aV7tvbHJx3/v87H2b9d32pwe8rO+2i665vu+20vLIzPsi4gjgs8A1EXEmcC+wObA3cD5wcHUVSlIzGXQlqQYy87iIGAJOAt7VsepG4OSRQxp6iYi5PVZNX74KJal5HLogSTUQEf8EnA6cTNGTuzowE7gJ+FZE/Ht11UlSM9mjK0kVi4jdgE8BZ2TmBztWXRER+wDXAx+KiBMy86bR9pWZM3scYy7FrcskadKwR1eSqvf6cn7hyBWZ+QhwOcX3620nsihJajqDriRVb5Vy3usWYsPLH5+AWiSpNQy6klS9S8r5uyNio84VEfFaYCfgUeDSiS5MkprMMbqSVL3TKe6T+yrg2og4A7gD2IpiWEMAR2bmvdWVKEnNY9CVpIpl5uKIeB1wCPBmYB9gNeA+4Bzg85l5XoUlSlIjGXQlqQYy8wnguHKSJA2AY3QlSZLUSvboqquFs7fru+3/+/fT+m77xtXv6avdE9n/72CHrHNd322vO3mDvtsO9X8KJElSDdmjK0mSpFayR1eSJoltNprK3Dmzqi5DkiaMPbqSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVvBhNkiaJebfNZ9qRZ1dy7CEvgpNUAXt0JUmS1EoGXUmSJLWSQVeSJEmt5BhddfXEav3/DtTvY33r4J82OL/vtn/zln/su+06p/xqLOVIkqRxZI+uJNVARLw1InIp06Kq65SkJrFHV5Lq4SrgmB7rdgH2AH48YdVIUgsYdCWpBjLzKoqw+zQRMTw25isTVY8ktYFDFySpxiJiG+AVwG1ANTfBlaSGMuhKUr0dXM6/lpmO0ZWkZeDQBUmqqYiYAhwILAZO7HObuT1WTR9UXZLUFPboSlJ9HQCsDfw4M2+puBZJahx7dCWpvt5dzr/c7waZObPb8rKnd8YgipKkprBHV5JqKCK2BnYEbgXOqbgcSWokg64k1ZMXoUnScnLoghpv5yv/ru+2n9n6u323nfqHhWMpR1puEbEqcBDFRWhfq7gcSWose3QlqX72B9YBzvEiNEkaO4OuJNXP8EVoPglNkpaDQVeSaiQitgJ2xovQJGm5OUZXkmokM68Fouo6JKkN7NGVJElSKxl0JUmS1EoOXZCkSWKbjaYyd86sqsuQpAljj64kSZJayaArSZKkVjLoSpIkqZUco6ta2uLcg/tv+/b/7bvtsby077YrcFXfbSVJUv3YoytJkqRWskdXkiaJebfNZ9qRZ4/b/oe8o4OkmrFHV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5JqJCJ2iYjvR8TtEfFYOT8vIl5XdW2S1DTedUGSaiIiPgp8ArgHOAu4HVgP2BbYDTinsuIkqYEMupJUAxGxP0XI/Smwb2YuGLH+GZUUJkkN5tAFSapYRKwAfAp4BPjbkSEXIDOfmPDCJKnh7NGVpOrtCGwKnA7cHxGzgG2AR4HLM/NXVRYnSU1l0JWk6r28nN8JXAG8qHNlRFwMvCkz717ajiJibo9V05erQklqIIcuSFL11i/n7wGmAK8C1qTo1T0X2BX4XjWlSVJz2aMrSdVbsZwHRc/t1eXr30XEPsD1wCsjYoelDWPIzJndlpc9vTMGVbAkNYE9upJUvfvL+U0dIReAzFxI0asLsN2EViVJDWfQlaTqXVfOH+ixfjgITxn/UiSpPQy6klS9i4EngRdExMpd1m9TzocmrCJJagGDriRVLDPvAU4DpgIf61wXEa8G/gqYD/xk4quTpObyYjRJqocPAtsDH4mIXYHLgU2AfYBFwLsy84HqypOk5jHoSlINZOZdEbE98FGKcPsKYAFwNvBvmXlZlfVJUhMZdCWpJjLzPoqe3Q9WXYsktYFjdCVJktRKBl1JkiS1kkMXJGmS2GajqcydM6vqMiRpwtijK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFbyrguSNEnMu20+0448e0KPOeRdHiRVyB5dSZIktZJBV5IkSa1k0JUkSVIrGXQlqQYiYigissd0R9X1SVITeTGaJNXHfOC4LssfmuA6JKkVDLqSVB8PZObRVRchSW3h0AVJkiS1kj26klQfq0TEgcDGwMPAb4GLM3NRtWVJUjMZdCWpPjYEvjli2c0R8bbM/HkVBUlSkxl0Jakevg5cAvwOWABsBvw/4N3AjyNih8y8emk7iYi5PVZNH1ShktQUBl1JqoHMPGbEonnAeyLiIeBDwNHAPhNdlyQ1mUFXkurtBIqgu2s/jTNzZrflZU/vjAHWJUm1510XJKne7irnq1dahSQ1kD26Wm4rjMPvS7ttfV3fbe981rP6brvo7rvHUo5UpR3K+U2VViFJDWSPriRVLCJeGBHrdlm+CfDF8uWpE1uVJDWfPbqSVL39gSMj4kLgZoq7LmwOzAJWBc4BPl1deZLUTAZdSarehcCWwLYUQxVWBx4AfkFxX91vZmZWVp0kNZRBV5IqVj4MwgdCSNKAOUZXkiRJrWTQlSRJUisZdCVJktRKjtGVpElim42mMnfOrKrLkKQJY4+uJEmSWskeXS23xSwe+D6/8ryL+m6793MO7H/HPhlNkqRJwx5dSZIktZJBV5IkSa3k0AVJmiTm3TafaUeePa7HGPJiN0k1Yo+uJEmSWsmgK0mSpFYy6EqSJKmVDLqSJElqJYOuJNVURBwUEVlO76y6HklqGoOuJNVQRDwP+ALwUNW1SFJTGXQlqWYiIoCvA/cCJ1RcjiQ1lvfRVeM9+uw1+m678tXjWIg0OIcCewC7lXNJ0hjYoytJNRIRWwFzgOMz8+Kq65GkJrNHV5JqIiJWAr4J/An48Bj3MbfHquljrUuSmsqgK0n18TFgW2DnzFxYdTGS1HQGXUmqgYjYjqIX9zOZ+aux7iczZ/bY/1xgxlj3K0lN5BhdSapYx5CF64GjKi5HklrDoCtJ1VsD2ALYCni04yERCXy8bPPVctlxVRUpSU3j0AVJqt5jwNd6rJtBMW73F8B1wJiHNUjSZGPQlaSKlReedX3Eb0QcTRF0T8nMEyeyLklqOocuSJIkqZUMupIkSWolhy6oq3Wuuq/vtv95/5Z9tz1knevGUs6oVv7H2/tv/JOBH14aV5l5NHB0xWVIUiPZoytJkqRWMuhKkiSplRy6IEmTxDYbTWXunFlVlyFJE8YeXUmSJLWSQVeSJEmtZNCVJElSKxl0JUmS1EoGXUmSJLWSd12QpEli3m3zmXbk2RNyrCHv7iCpBuzRlSRJUivZo6uuFl1zfd9tzztoh77bvu+sG/pq94xYse99njX9h3233fVvDum77VrfvqzvtpIkqX7s0ZUkSVIrGXQlSZLUSgZdSaqBiPhURPwsIm6JiIURcV9EXBkRH4+IZ1ZdnyQ1kUFXkurhMGB14HzgeOBbwJPA0cBvI+J51ZUmSc3kxWiSVA9rZeajIxdGxLHAh4F/Bv5hwquSpAazR1eSaqBbyC19t5y/YKJqkaS2MOhKUr29oZz/ttIqJKmBHLogSTUSEYcDawBTgZcBO1OE3Dl9bj+3x6rpAylQkhrEoCtJ9XI4sEHH658Ab83MuyuqR5Iay6ArSTWSmRsCRMQGwI4UPblXRsTrM/OKPraf2W152dM7Y5C1SlLdGXS13FZY8Ejfbb//0Hp9tdtvjXv63udiFvfd9sWHXd1326Fv991UGrjMvBM4IyKuAK4HvgFsU21VktQsXowmSTWWmX8ErgFeGBH9/aYoSQIMupLUBM8p54sqrUKSGsagK0kVi4jpEbFhl+UrlA+MWB+4NDPvn/jqJKm5HKMrSdV7DfAfEXEx8AfgXoo7L7wS2Ay4A3hXdeVJUjMZdCWpej8FvgLsBLwEWBt4mOIitG8Cn8/M+yqrTpIayqArSRXLzHnAIVXXIUlt4xhdSZIktZJBV5IkSa3k0AVJmiS22Wgqc+fMqroMSZow9uhKkiSplezR1XJbdOPNfbf96I/+uq92+/3Nf461nFHtPvX3fbf9xgYv77vtojvvGks5kiRpHNmjK0mSpFYy6EqSJKmVDLqSJElqJcfoStIkMe+2+Uw78uwJPeaQd3mQVCF7dCVJktRKBl1JkiS1kkFXkiRJrWTQlaSKRcQzI+KdEXFGRNwYEQsjYn5E/CIi3hERfq+WpDHwYjRJqt7+wJeA24ELgT8BGwD7AicCr42I/TMzqytRkprHoCtJ1bse2Bs4OzMXDy+MiA8DlwP7UYTe71dTniQ1k0FXE2rzwy/rq90uW/f3qGCAn7/k23233WeN/h/Ve9YZj/fd9u4d+24qPU1mXtBj+R0RcQJwLLAbBl1JWiaO+5KkenuinD9ZaRWS1EAGXUmqqYhYCfj78uVPqqxFkprIoQuSVF9zgG2AczLz3H42iIi5PVZNH1hVktQQ9uhKUg1FxKHAh4DfAwdVXI4kNZI9upJUMxFxCHA8cA2wZ2be1++2mTmzxz7nAjMGU6EkNYM9upJUIxHxAeCLwDxg98y8o9qKJKm5DLqSVBMRcQTwOeAqipDb//3wJElPY9CVpBqIiKMoLj6bSzFc4Z6KS5KkxnOMriRVLCLeAvwLsAi4BDg0IkY2G8rMkye4NElqNIOuJFVv03K+IvCBHm1+Dpw8EcVIUlsYdFVLU193Y99tv/37jfpu+3dr3t53269v8rO+2+45+719t51y5uV9t9XkkJlHA0dXXIYktY5jdCVJktRKBl1JkiS1kkFXkiRJreQYXUmaJLbZaCpz58yqugxJmjD26EqSJKmVDLqSJElqJYOuJEmSWsmgK0mSpFbyYjRJmiTm3TafaUeePSHHGvKiN0k1YI+uJEmSWskeXTXel4/Zt++2mx17Qt9tt1/lib7brnjInX23fXLBzL7brvSzuX23lSRJT2WPriRJklrJoCtJkqRWMuhKUg1ExJsi4gsRcUlEPBgRGRGnVl2XJDWZY3QlqR4+CrwEeAi4FZhebTmS1Hz26EpSPRwGbAGsBby34lokqRXs0ZWkGsjMC4f/HRFVliJJrWGPriRJklrJHl1JapGI6HXzZcf8Spp07NGVJElSK9mjK0ktkpldH71X9vTOmOByJKlSBl013lrfvqzvtkfwnr7bfulfj++77Xlb/6Dvtr/66op9tz3mne/ou+1KF/i4YEmSOjl0QZIkSa1k0JUkSVIrGXQlSZLUSo7RlaQaiIjZwOzy5YblfIeIOLn89z2ZefgElyVJjWbQlaR6eCnwlhHLNisngD8CBl1JWgYOXZCkGsjMozMzRpmmVV2jJDWNQVeSJEmtZNCVJElSKzlGV5ImiW02msrcObOqLkOSJoxBV5PKsjxF7dBH3td328cPvq/vtvttfGXfbVe5Y0HfbRf13VKSpMnBoQuSJElqJYOuJEmSWsmgK0mSpFYy6EqSJKmVvBhNkiaJebfNZ9qRZ0/4cYe804OkitijK0mSpFYy6EqSJKmVDLqSJElqJYOuJEmSWsmgK0k1ERHPjYiTIuLPEfFYRAxFxHERsU7VtUlSE3nXBamHKT+8fBna9r/fn7LmMlRx/TK0VZNFxObApcD6wA+B3wPbAe8HXhMRO2XmvRWWKEmNY4+uJNXDf1GE3EMzc3ZmHpmZewCfA7YEjq20OklqIIOuJFUsIjYD9gKGgP8csfrjwMPAQRGx+gSXJkmNZtCVpOrtUc7Py8zFnSsycwHwS2A14BUTXZgkNZljdCWpeluW816Dsm+g6PHdAvjZaDuKiLk9Vk0fW2mS1Fz26EpS9aaW8/k91g8vX3v8S5Gk9rBHV5LqL8p5Lq1hZs7suoOip3fGIIuSpLqzR1eSqjfcYzu1x/q1RrSTJPXBoCtJ1buunG/RY/0Lyrk3VpakZWDQlaTqXVjO94qIp3xfjog1gZ2AhcBlE12YJDWZQVeSKpaZfwDOA6YBh4xYfQywOvCNzHx4gkuTpEbzYjRJqod/oHgE8OcjYk/gWmB7YHeKIQsfqbA2SWoke3QlqQbKXt2XASdTBNwPAZsDnwd2yMx7q6tOkprJHl1JqonMvAV4W9V1SFJb2KMrSZKkVjLoSpIkqZUcuiBJk8Q2G01l7pxZVZchSRPGHl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKBl1JkiS1kkFXkiRJrWTQlSRJUisZdCVJktRKK1VdgCRpQky79tprmTlzZtV1SNIyufbaawGmjWVbg64kTQ5rLFy4cNEVV1xxddWF1Mj0cv77SquoF8/J03lOnm6iz8k04MGxbGjQlaTJYR5AZtqlW4qIueA56eQ5eTrPydM16Zw4RleSJEmtNOYe3fMXfy8GWYgkSZI0SPboSpIkqZUMupIkSWolg64kSZJaKTKz6hokSZKkgbNHV5IkSa1k0JUkSVIrGXQlSZLUSgZdSZIktZJBV5IkSa1k0JUkSVIrGXQlSZLUSgZdSaqxiHhuRJwUEX+OiMciYigijouIdcZ7PxGxY0ScExH3RcQjEfHbiPhARKy4/O9s7Jb3nETEMyPinRFxRkTcGBELI2J+RPwiIt4REU/72RgR0yIiR5m+M/h32r9BfE7KbXq9vztG2a6tn5O3LuVrnhGxaMQ2tf2cRMSbIuILEXFJRDxY1nPqGPfVmO8nPjBCkmoqIjYHLgXWB34I/B7YDtgduA7YKTPvHY/9RMQbge8DjwKnAfcBbwC2BE7PzP0H8BaX2SDOSUS8B/gScDtwIfAnYANgX2AqxfvePzt+QEbENOBm4GrgzC67nZeZpy/HWxuzAX5OhoC1geO6rH4oMz/dZZs2f05eCszusXoXYA/g7Mx8fcc206jv5+Qq4CXAQ8CtwHTgW5l54DLup1nfTzLTycnJyamGE3AukMD7Riz/bLn8hPHYD7AWcBfwGPCyjuWrUvyAS+DNTT0nFAHlDcAKI5ZvSBF6E9hvxLpp5fKTq/5cjOPnZAgYWobjtvpzspT9/6rcz94N+pzsDrwACGC3ss5Tx/vcVv05qfzEOzk5OTk9fQI2K38A3NwlkK1J0SvzMLD6oPcDvL3c5pQu+9ujXPfzpp6TpRzjw+UxvjBieS0DzCDPyRiC7qT8nADblPu/FVixCZ+TLu9hTEG3id9PHKMrSfW0Rzk/LzMXd67IzAXAL4HVgFeMw36Gt/lJl/1dDDwC7BgRqyztTQzYoM7JaJ4o50/2WP+ciDg4Ij5czl+8HMcahEGfk1Ui4sDy/b0/InYfZQzlZP2cHFzOv5aZi3q0qdvnZFAa9/3EoCtJ9bRlOb++x/obyvkW47Cfnttk5pMUvTkrUfTuTKRBnZOuImIl4O/Ll91+KAO8GjgBOLacXx0RF0bExmM55gAM+pxsCHyT4v0dB1wA3BARr1yWY7f1cxIRU4ADgcXAiaM0rdvnZFAa9/3EoCtJ9TS1nM/vsX54+drjsJ9BHXvQxruuORR/lj4nM88dse4R4BPATGCdcnolxcVsuwE/i4jVx3jc5THIc/J1YE+KsLs68CLgyxR/jv9xRLxkHI89SONZ1wHldj/OzFu6rK/r52RQGvf9xKArSc0U5Xx5b50zlv0M6tiDNua6IuJQ4EMUV5AfNHJ9Zt6VmR/LzCsy84FyuhjYC/g18HzgnWMvfdz0fU4y85jMvCAz78zMRzJzXma+h+IioynA0eN17Am2PHW9u5x/udvKBn9OBqV2308MupJUT8O9HFN7rF9rRLtB7mdQxx60cakrIg4BjgeuAXbPzPv63bb80+vwn7B3XZbjDshEfK1OKOcj399k+5xsDexIcRHaOcuybQ0+J4PSuO8nBl1JqqfrynmvcYQvKOe9xsotz356blOOY92U4mKtm5Zy7EEb1Dn5i4j4APBFYB5FyO35YIRR3F3Oq/iT9MDPSRd3lfOR72/SfE5K/VyENpoqPyeD0rjvJwZdSaqnC8v5XjHiSV0RsSawE7AQuGwc9nNBOX9Nl/3tSnFV9aWZ+djS3sSADeqcDG9zBPA54CqKkHvX6Fv0NHyF+UQHOhjwOelhh3I+8v1Nis9Jud2qFENaFgNfG2NdVX5OBqVx308MupJUQ5n5B+A8iguBDhmx+hiKXqFvZObDABHxjIiYXj61aMz7KZ0O3AO8OSJeNryw/GH/yfLll8b85sZoUOekXHcUxcVnc4E9M/Oe0Y4dEdtHxMpdlu8BHFa+HNPjVJfHoM5JRLwwItYduf+I2ISixxue/v5a/znpsD/FhWXn9LgIjXJftfycLKs2fT/xEcCSVFNdHrV5LbA9xROOrgd2zPJRmx2PHv1jZk4b6346tplN8QPqUeA7FI/s3JvykZ3AAVnBD5BBnJOIeAtwMrAI+ALdxwYOZebJHdtcBLwQuIhijCbAi1lyj9CjMvOTVGBA5+Ro4EiKHrubgQXA5sAsiidYnQPsk5mPjzj2bFr6ORmxv0uAnSmehPajUY57EfX9nMxmySONNwT+iqJ3+ZJy2T2ZeXjZdhpt+X4yXk+icHJycnJa/gl4HsVtn24HHgf+SHHh1Loj2k2juGp5aHn2M2KbnSgCzv0Uf478P4peqRUH9f6qOCcUdw/IpUwXjdjmHcBZFE8Pe4jicaZ/Ak4Ddmn654TiFljfprjrxAMUD864Gzif4t7CMdk+Jx3rtyrX37K091Tnz0kfn/uhjrat+X5ij64kSZJayTG6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaiWDriRJklrJoCtJkqRWMuhKkiSplQy6kiRJaqX/D7GjSI6+VclUAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab some data \n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# Resize images into a 1D vector, new shape is (batch size, color channels, image pixels) \n",
    "images.resize_(64, 1, 784)\n",
    "# or images.resize_(images.shape[0], 1, 784) to not automatically get batch size\n",
    "\n",
    "# Forward pass through the network\n",
    "img_idx = 0\n",
    "ps = model.forward(images[img_idx,:])\n",
    "\n",
    "img = images[img_idx]\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our network has basically no idea what this digit is. It's because we haven't trained it yet, all the weights are random!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Neural Networks\n",
    "\n",
    "The network we built isn't so smart, it doesn't know anything about our handwritten digits. Neural networks with non-linear activations work like universal function approximators. There is some function that maps your input to the output. For example, images of handwritten digits to class probabilities. The power of neural networks is that we can train them to approximate this function, and basically any function given enough data and compute time.\n",
    "\n",
    "<img src=\"assets/function_approx.png\" width=500px>\n",
    "\n",
    "At first the network is naive, it doesn't know the function mapping the inputs to the outputs. We train the network by showing it examples of real data, then adjusting the network parameters such that it approximates this function.\n",
    "\n",
    "To find these parameters, we need to know how poorly the network is predicting the real outputs. For this we calculate a **loss function** (also called the cost), a measure of our prediction error. For example, the mean squared loss is often used in regression and binary classification problems\n",
    "\n",
    "$$\n",
    "\\ell = \\frac{1}{2n}\\sum_i^n{\\left(y_i - \\hat{y}_i\\right)^2}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of training examples, $y_i$ are the true labels, and $\\hat{y}_i$ are the predicted labels.\n",
    "\n",
    "By minimizing this loss with respect to the network parameters, we can find configurations where the loss is at a minimum and the network is able to predict the correct labels with high accuracy. We find this minimum using a process called **gradient descent**. The gradient is the slope of the loss function and points in the direction of fastest change. To get to the minimum in the least amount of time, we then want to follow the gradient (downwards). You can think of this like descending a mountain by following the steepest slope to the base.\n",
    "\n",
    "<img src='assets/gradient_descent.png' width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "For single layer networks, gradient descent is simple to implement. However, it's more complicated for deeper, multilayer neural networks like the one we've built. Complicated enough that it took about 30 years before researchers figured out how to train multilayer networks, although it's straightforward once you learn about it. \n",
    "\n",
    "This is done through **backpropagation** which is really just an application of the chain rule from calculus. It's easiest to understand if we convert a two layer network into a graph representation.\n",
    "\n",
    "<img src='assets/w1_backprop_graph.png' width=400px>\n",
    "\n",
    "In the forward pass through the network, our data and operations go from right to left here. To train the weights with gradient descent, we propagate the gradient of the cost backwards through the network. Mathematically, this is really just calculating the gradient of the loss with respect to the weights using the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\ell}{\\partial w_1} = \\frac{\\partial l_1}{\\partial w_1} \\frac{\\partial s}{\\partial l_1} \\frac{\\partial l_2}{\\partial s} \\frac{\\partial \\ell}{\\partial l_2}\n",
    "$$\n",
    "\n",
    "We update our weights using this gradient with some learning rate $\\alpha$. \n",
    "\n",
    "$$\n",
    "w^\\prime = w - \\alpha \\frac{\\partial \\ell}{\\partial w}\n",
    "$$\n",
    "\n",
    "The learning rate is set such that the weight update steps are small enough that the iterative method settles in a minimum.\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "\n",
    "Torch provides a module, `autograd`, for automatically calculating the gradient of tensors. It does this by keeping track of operations performed on tensors. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set `requires_grad` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time with `x.requires_grad_(True)`.\n",
    "\n",
    "You can turn off gradients for a block of code with the `torch.no_grad()` content:\n",
    "```python\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n",
    "```\n",
    "\n",
    "Also, you can turn on or off gradients altogether with `torch.set_grad_enabled(True|False)`.\n",
    "\n",
    "The gradients are computed with respect to some variable `z` with `z.backward()`. This does a backward pass through the operations that created `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1180, -0.3411],\n",
      "        [ 1.6020, -0.6616]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0139, 0.1164],\n",
      "        [2.5665, 0.4377]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see the operation that created `y`, a power operation `PowBackward0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x7f30b3b39e80>\n"
     ]
    }
   ],
   "source": [
    "## grad_fn shows the function that generated this variable\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autgrad module keeps track of these operations and knows how to calculate the gradient for each one. In this way, it's able to calculate the gradients for a chain of operations, with respect to any one tensor. Let's reduce the tensor `y` to a scalar value, the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7836, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the gradients for `x` and `y` but they are empty currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the gradients, you need to run the `.backward` method on a Variable, `z` for example. This will calculate the gradient for `z` with respect to `x`\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x}\\left[\\frac{1}{n}\\sum_i^n x_i^2\\right] = \\frac{x}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0590, -0.1706],\n",
      "        [ 0.8010, -0.3308]])\n",
      "tensor([[-0.0590, -0.1706],\n",
      "        [ 0.8010, -0.3308]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These gradients calculations are particularly useful for neural networks. For training we need the gradients of the weights with respect to the cost. With PyTorch, we run data forward through the network to calculate the cost, then, go backwards to calculate the gradients with respect to the cost. Once we have the gradients we can make a gradient descent step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll build a network with `nn.Sequential` here. Only difference from the last part is I'm not actually using softmax on the output, but instead just using the raw output from the last layer. This is because the output from softmax is a probability distribution. Often, the output will have values really close to zero or really close to one. Due to [inaccuracies with representing numbers as floating points](https://docs.python.org/3/tutorial/floatingpoint.html), computations with a softmax output can lose accuracy and become unstable. To get around this, we'll use the raw output, called the **logits**, to calculate the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "hidden_sizes = [128, 64]\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, hidden_sizes[0])),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('logits', nn.Linear(hidden_sizes[1], output_size))]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "The first thing we need to do for training is define our loss function. In PyTorch, you'll usually see this as `criterion`. Here we're using softmax output, so we want to use `criterion = nn.CrossEntropyLoss()` as our loss. Later when training, you use `loss = criterion(output, targets)` to calculate the actual loss.\n",
    "\n",
    "We also need to define the optimizer we're using, SGD or Adam, or something along those lines. Here I'll just use SGD with `torch.optim.SGD`, passing in the network parameters and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider just one learning step before looping through all the data. The general process with PyTorch:\n",
    "\n",
    "* Make a forward pass through the network to get the logits \n",
    "* Use the logits to calculate the loss\n",
    "* Perform a backward pass through the network with `loss.backward()` to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights\n",
    "\n",
    "Below I'll go through one training step and print out the weights and gradients so you can see how it changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0302,  0.0216,  0.0057,  ...,  0.0189, -0.0314,  0.0299],\n",
      "        [ 0.0240,  0.0229, -0.0042,  ..., -0.0013,  0.0203,  0.0357],\n",
      "        [ 0.0287, -0.0229, -0.0332,  ..., -0.0190, -0.0032,  0.0333],\n",
      "        ...,\n",
      "        [-0.0113,  0.0195, -0.0258,  ..., -0.0237, -0.0062,  0.0135],\n",
      "        [-0.0230, -0.0324, -0.0188,  ...,  0.0284, -0.0066, -0.0086],\n",
      "        [ 0.0084,  0.0131, -0.0299,  ..., -0.0306,  0.0118,  0.0187]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[ 0.0006,  0.0006,  0.0006,  ...,  0.0006,  0.0006,  0.0006],\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [ 0.0005,  0.0005,  0.0005,  ...,  0.0005,  0.0005,  0.0005],\n",
      "        ...,\n",
      "        [-0.0010, -0.0010, -0.0010,  ..., -0.0010, -0.0010, -0.0010],\n",
      "        [-0.0005, -0.0005, -0.0005,  ..., -0.0005, -0.0005, -0.0005],\n",
      "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.fc1.weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print('Gradient -', model.fc1.weight.grad)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0302,  0.0215,  0.0057,  ...,  0.0189, -0.0314,  0.0299],\n",
      "        [ 0.0240,  0.0229, -0.0042,  ..., -0.0013,  0.0203,  0.0357],\n",
      "        [ 0.0287, -0.0229, -0.0332,  ..., -0.0190, -0.0032,  0.0333],\n",
      "        ...,\n",
      "        [-0.0113,  0.0195, -0.0258,  ..., -0.0237, -0.0062,  0.0135],\n",
      "        [-0.0230, -0.0324, -0.0188,  ...,  0.0284, -0.0066, -0.0086],\n",
      "        [ 0.0084,  0.0131, -0.0298,  ..., -0.0306,  0.0118,  0.0187]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Updated weights - ', model.fc1.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for real\n",
    "\n",
    "Now we'll put this algorithm into a loop so we can go through all the images. This is fairly straightforward. We'll loop through the mini-batches in our dataset, pass the data through the network to calculate the losses, get the gradients, then run the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "\tIteration: 0\t Loss: 0.0577\n",
      "\tIteration: 40\t Loss: 2.2901\n",
      "\tIteration: 80\t Loss: 2.2719\n",
      "\tIteration: 120\t Loss: 2.2538\n",
      "\tIteration: 160\t Loss: 2.2365\n",
      "\tIteration: 200\t Loss: 2.2115\n",
      "\tIteration: 240\t Loss: 2.1935\n",
      "\tIteration: 280\t Loss: 2.1806\n",
      "\tIteration: 320\t Loss: 2.1443\n",
      "\tIteration: 360\t Loss: 2.1230\n",
      "\tIteration: 400\t Loss: 2.0907\n",
      "\tIteration: 440\t Loss: 2.0582\n",
      "\tIteration: 480\t Loss: 2.0117\n",
      "\tIteration: 520\t Loss: 1.9794\n",
      "\tIteration: 560\t Loss: 1.9281\n",
      "\tIteration: 600\t Loss: 1.8770\n",
      "\tIteration: 640\t Loss: 1.8208\n",
      "\tIteration: 680\t Loss: 1.7786\n",
      "\tIteration: 720\t Loss: 1.7033\n",
      "\tIteration: 760\t Loss: 1.6508\n",
      "\tIteration: 800\t Loss: 1.5676\n",
      "\tIteration: 840\t Loss: 1.5173\n",
      "\tIteration: 880\t Loss: 1.4546\n",
      "\tIteration: 920\t Loss: 1.3854\n",
      "Epoch: 2/3\n",
      "\tIteration: 0\t Loss: 0.0317\n",
      "\tIteration: 40\t Loss: 1.2850\n",
      "\tIteration: 80\t Loss: 1.2178\n",
      "\tIteration: 120\t Loss: 1.1744\n",
      "\tIteration: 160\t Loss: 1.1349\n",
      "\tIteration: 200\t Loss: 1.0717\n",
      "\tIteration: 240\t Loss: 0.9999\n",
      "\tIteration: 280\t Loss: 0.9916\n",
      "\tIteration: 320\t Loss: 0.9296\n",
      "\tIteration: 360\t Loss: 0.8857\n",
      "\tIteration: 400\t Loss: 0.8812\n",
      "\tIteration: 440\t Loss: 0.8212\n",
      "\tIteration: 480\t Loss: 0.8254\n",
      "\tIteration: 520\t Loss: 0.7772\n",
      "\tIteration: 560\t Loss: 0.7648\n",
      "\tIteration: 600\t Loss: 0.7597\n",
      "\tIteration: 640\t Loss: 0.7141\n",
      "\tIteration: 680\t Loss: 0.7083\n",
      "\tIteration: 720\t Loss: 0.7022\n",
      "\tIteration: 760\t Loss: 0.6892\n",
      "\tIteration: 800\t Loss: 0.6523\n",
      "\tIteration: 840\t Loss: 0.6588\n",
      "\tIteration: 880\t Loss: 0.6332\n",
      "\tIteration: 920\t Loss: 0.6231\n",
      "Epoch: 3/3\n",
      "\tIteration: 0\t Loss: 0.0124\n",
      "\tIteration: 40\t Loss: 0.6082\n",
      "\tIteration: 80\t Loss: 0.5661\n",
      "\tIteration: 120\t Loss: 0.6171\n",
      "\tIteration: 160\t Loss: 0.5691\n",
      "\tIteration: 200\t Loss: 0.5649\n",
      "\tIteration: 240\t Loss: 0.5817\n",
      "\tIteration: 280\t Loss: 0.5649\n",
      "\tIteration: 320\t Loss: 0.5943\n",
      "\tIteration: 360\t Loss: 0.5367\n",
      "\tIteration: 400\t Loss: 0.5241\n",
      "\tIteration: 440\t Loss: 0.5196\n",
      "\tIteration: 480\t Loss: 0.5374\n",
      "\tIteration: 520\t Loss: 0.5273\n",
      "\tIteration: 560\t Loss: 0.5307\n",
      "\tIteration: 600\t Loss: 0.4972\n",
      "\tIteration: 640\t Loss: 0.5087\n",
      "\tIteration: 680\t Loss: 0.4981\n",
      "\tIteration: 720\t Loss: 0.5208\n",
      "\tIteration: 760\t Loss: 0.4675\n",
      "\tIteration: 800\t Loss: 0.4719\n",
      "\tIteration: 840\t Loss: 0.5105\n",
      "\tIteration: 880\t Loss: 0.4546\n",
      "\tIteration: 920\t Loss: 0.4918\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "print_every = 40\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"Epoch: {e+1}/{epochs}\")\n",
    "\n",
    "    for i, (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images.resize_(images.size()[0], 784)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model.forward(images)   # 1) Forward pass\n",
    "        loss = criterion(output, labels) # 2) Compute loss\n",
    "        loss.backward()                  # 3) Backward pass\n",
    "        optimizer.step()                 # 4) Update model\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % print_every == 0:\n",
    "            print(f\"\\tIteration: {i}\\t Loss: {running_loss/print_every:.4f}\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the network trained, we can check out it's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our network is brilliant. It can accurately predict the digits in our images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "    <h2 align=\"center\" style=\"color:#01ff84\">MNIST Clasification: Exercise</h2>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 1:</h3>\n",
    "  <p>Now it's your turn to build a simple network, use any method I've covered so far. In the next notebook, you'll learn how to train a network so it can make good predictions.</p>\n",
    "  <p>Build a network to classify the MNIST images with 3 hidden layers. Use 400 units in the first hidden layer, 200 units in the second layer, and 100 units in the third layer. Each hidden layer should have a ReLU activation function, and use softmax on the output layer.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your network here\n",
    "# Hyperparameters for our network\n",
    "input_size   = 784\n",
    "output_size  = 10\n",
    "\n",
    "# Build a feed-forward network\n",
    "model = nn.Sequential(OrderedDict([\n",
    "          ('fc1', nn.Linear(input_size, 400)),\n",
    "          ('relu1', nn.ReLU()),\n",
    "          ('fc2', nn.Linear(400, 200)),\n",
    "          ('relu2', nn.ReLU()),\n",
    "          ('fc3', nn.Linear(200, 100)),\n",
    "          ('relu3', nn.ReLU()),\n",
    "          ('logits', nn.Linear(100, output_size))]))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works\n",
    "# Forward pass through the network and display output\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "ps = model.forward(images[0,:])\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 2:</h3>\n",
    "  <p>Train your network implementing the Pytorch training loop and <strong style=\"color:#01ff84\">after each epoch, use the model for predicting the test (validation) MNIST data.</strong></p>\n",
    "  <p>Note: If your model does not fit with the final softmax layer, you can remove this layer.</p>\n",
    "  <p>Hint: <a href=\"https://discuss.pytorch.org/t/training-loop-checking-validation-accuracy/78399\">Training loop checking validation accuracy\n",
    "</a></p>\n",
    "  <p>Research about <code>model.train()</code>, <code>model.eval()</code> and <code>with torch.no_grad()</code> in Pytorch.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "loss:   0.04707673490047455\n",
      "loss:   1.7605353623628617\n",
      "loss:   1.7689544528722763\n",
      "loss:   1.756615823507309\n",
      "loss:   1.7835780739784242\n",
      "loss:   1.7472037255764008\n",
      "loss:   1.74878072142601\n",
      "loss:   1.720134550333023\n",
      "loss:   1.7488706082105636\n",
      "loss:   1.7516052335500718\n",
      "loss:   1.720803800225258\n",
      "loss:   1.723985132575035\n",
      "loss:   1.7222894072532653\n",
      "loss:   1.7144308894872666\n",
      "loss:   1.695562681555748\n",
      "loss:   1.713572657108307\n",
      "loss:   1.681793937087059\n",
      "loss:   1.6890636682510376\n",
      "loss:   1.672680962085724\n",
      "loss:   1.6826154291629791\n",
      "loss:   1.6715954482555389\n",
      "loss:   1.6541271537542344\n",
      "loss:   1.6398150891065597\n",
      "loss:   1.6579610973596572\n",
      "epoch: 1\n",
      "loss:   0.04042757749557495\n",
      "loss:   1.6408668726682663\n",
      "loss:   1.6118725270032883\n",
      "loss:   1.6291299313306808\n",
      "loss:   1.6121486574411392\n",
      "loss:   1.608968898653984\n",
      "loss:   1.5974507480859756\n",
      "loss:   1.5982315003871919\n",
      "loss:   1.6018897980451583\n",
      "loss:   1.5863590508699417\n",
      "loss:   1.5971818655729293\n",
      "loss:   1.5908848881721496\n",
      "loss:   1.5728263586759568\n",
      "loss:   1.5747722446918488\n",
      "loss:   1.5577168852090835\n",
      "loss:   1.5501091122627257\n",
      "loss:   1.5476092606782914\n",
      "loss:   1.5478267073631287\n",
      "loss:   1.5402950197458267\n",
      "loss:   1.5479606568813324\n",
      "loss:   1.5320524215698241\n",
      "loss:   1.5028050303459168\n",
      "loss:   1.5144310265779495\n",
      "loss:   1.5064923375844956\n",
      "epoch: 2\n",
      "loss:   0.03845030069351196\n",
      "loss:   1.4769299566745757\n",
      "loss:   1.4756472080945968\n",
      "loss:   1.4660290867090224\n",
      "loss:   1.4611983567476272\n",
      "loss:   1.4809119880199433\n",
      "loss:   1.4634124338626862\n",
      "loss:   1.464648675918579\n",
      "loss:   1.452208438515663\n",
      "loss:   1.4356450468301774\n",
      "loss:   1.420769417285919\n",
      "loss:   1.4078071385622024\n",
      "loss:   1.4104877084493637\n",
      "loss:   1.430719530582428\n",
      "loss:   1.418435400724411\n",
      "loss:   1.4266581118106842\n",
      "loss:   1.4072718888521194\n",
      "loss:   1.3873656421899796\n",
      "loss:   1.4107194036245345\n",
      "loss:   1.377791592478752\n",
      "loss:   1.3762317538261413\n",
      "loss:   1.3693394303321837\n",
      "loss:   1.366815909743309\n",
      "loss:   1.3793529331684113\n",
      "epoch: 3\n",
      "loss:   0.03508723974227905\n",
      "loss:   1.3417405098676682\n",
      "loss:   1.3322534680366516\n",
      "loss:   1.34430151283741\n",
      "loss:   1.3196030020713807\n",
      "loss:   1.3219382613897324\n",
      "loss:   1.3043746858835221\n",
      "loss:   1.285802486538887\n",
      "loss:   1.3028825253248215\n",
      "loss:   1.3000172346830368\n",
      "loss:   1.305276307463646\n",
      "loss:   1.3264337331056595\n",
      "loss:   1.296068835258484\n",
      "loss:   1.2856692314147948\n",
      "loss:   1.263523694872856\n",
      "loss:   1.253295260667801\n",
      "loss:   1.2614009529352188\n",
      "loss:   1.2554778680205345\n",
      "loss:   1.2586194038391114\n",
      "loss:   1.2427330285310745\n",
      "loss:   1.2208707362413407\n",
      "loss:   1.2244221419095993\n",
      "loss:   1.2205568909645081\n",
      "loss:   1.2210505455732346\n",
      "epoch: 4\n",
      "loss:   0.028085789084434508\n",
      "loss:   1.2091342478990554\n",
      "loss:   1.2051688045263291\n",
      "loss:   1.1952075511217117\n",
      "loss:   1.1990324914455415\n",
      "loss:   1.2200689733028411\n",
      "loss:   1.1754073739051818\n",
      "loss:   1.1878541857004166\n",
      "loss:   1.1722178682684898\n",
      "loss:   1.169778862595558\n",
      "loss:   1.1686742037534714\n",
      "loss:   1.1784392595291138\n",
      "loss:   1.140428227186203\n",
      "loss:   1.1357799738645553\n",
      "loss:   1.1423933163285256\n",
      "loss:   1.1202751740813255\n",
      "loss:   1.1203881427645683\n",
      "loss:   1.1208154454827308\n",
      "loss:   1.1131794184446335\n",
      "loss:   1.1232852920889855\n",
      "loss:   1.1163004904985427\n",
      "loss:   1.1052583128213882\n",
      "loss:   1.1137647315859796\n",
      "loss:   1.0997647613286972\n",
      "epoch: 5\n",
      "loss:   0.031197717785835265\n",
      "loss:   1.0784809917211533\n",
      "loss:   1.0927501931786536\n",
      "loss:   1.088884563744068\n",
      "loss:   1.0832405790686608\n",
      "loss:   1.0764694213867188\n",
      "loss:   1.0579209178686142\n",
      "loss:   1.0555545464158058\n",
      "loss:   1.0664871096611024\n",
      "loss:   1.0655187338590622\n",
      "loss:   1.040922960639\n",
      "loss:   1.075054132938385\n",
      "loss:   1.0466299787163735\n",
      "loss:   1.0396256744861603\n",
      "loss:   1.0415897950530053\n",
      "loss:   1.0112449645996093\n",
      "loss:   1.0249232783913613\n",
      "loss:   1.0089759930968285\n",
      "loss:   1.0131494104862213\n",
      "loss:   1.030995288491249\n",
      "loss:   1.0235298946499825\n",
      "loss:   1.0218675807118416\n",
      "loss:   0.9948141887784004\n",
      "loss:   1.017877733707428\n",
      "epoch: 6\n",
      "loss:   0.02652571499347687\n",
      "loss:   0.9872625544667244\n",
      "loss:   0.9856958031654358\n",
      "loss:   0.992789588868618\n",
      "loss:   0.9801140800118446\n",
      "loss:   0.971198408305645\n",
      "loss:   0.9800586149096489\n",
      "loss:   0.9740100517868996\n",
      "loss:   0.9807441234588623\n",
      "loss:   0.9492243334650994\n",
      "loss:   0.9629074618220329\n",
      "loss:   0.9719420522451401\n",
      "loss:   0.9545598924160004\n",
      "loss:   0.9251120775938034\n",
      "loss:   0.9459497347474098\n",
      "loss:   0.9699254304170608\n",
      "loss:   0.9338062107563019\n",
      "loss:   0.9483801260590553\n",
      "loss:   0.9186721250414849\n",
      "loss:   0.9469065234065056\n",
      "loss:   0.9342691391706467\n",
      "loss:   0.9313806757330895\n",
      "loss:   0.9088140860199928\n",
      "loss:   0.9265349626541137\n",
      "epoch: 7\n",
      "loss:   0.022227807343006133\n",
      "loss:   0.9035389170050621\n",
      "loss:   0.9061861380934715\n",
      "loss:   0.8964133769273758\n",
      "loss:   0.9127081215381623\n",
      "loss:   0.9070737212896347\n",
      "loss:   0.8970877841114998\n",
      "loss:   0.9202886700630188\n",
      "loss:   0.8938836365938186\n",
      "loss:   0.8954717233777046\n",
      "loss:   0.8807026103138924\n",
      "loss:   0.8961835712194443\n",
      "loss:   0.8667379796504975\n",
      "loss:   0.8858395189046859\n",
      "loss:   0.8680315300822258\n",
      "loss:   0.9012262836098671\n",
      "loss:   0.8545964986085892\n",
      "loss:   0.8713017001748085\n",
      "loss:   0.8509816661477089\n",
      "loss:   0.8548513814806938\n",
      "loss:   0.8647844880819321\n",
      "loss:   0.8477902635931969\n",
      "loss:   0.8678857252001763\n",
      "loss:   0.8428139626979828\n",
      "epoch: 8\n",
      "loss:   0.024124576151371\n",
      "loss:   0.852499620616436\n",
      "loss:   0.840335164964199\n",
      "loss:   0.869916270673275\n",
      "loss:   0.8250094056129456\n",
      "loss:   0.8499504059553147\n",
      "loss:   0.8486982077360153\n",
      "loss:   0.8269363671541214\n",
      "loss:   0.8375035732984543\n",
      "loss:   0.8159165725111961\n",
      "loss:   0.8446603700518608\n",
      "loss:   0.82751574665308\n",
      "loss:   0.8387587875127792\n",
      "loss:   0.8088583931326866\n",
      "loss:   0.8020253270864487\n",
      "loss:   0.8039117202162742\n",
      "loss:   0.8310128927230835\n",
      "loss:   0.8128440380096436\n",
      "loss:   0.7859473273158073\n",
      "loss:   0.8024804472923279\n",
      "loss:   0.7901066616177559\n",
      "loss:   0.7988440841436386\n",
      "loss:   0.7876605406403542\n",
      "loss:   0.8061438024044036\n",
      "epoch: 9\n",
      "loss:   0.02130342274904251\n",
      "loss:   0.7705874308943749\n",
      "loss:   0.7980172514915467\n",
      "loss:   0.815261147916317\n",
      "loss:   0.7700933307409287\n",
      "loss:   0.7705062896013259\n",
      "loss:   0.772160966694355\n",
      "loss:   0.7920974671840668\n",
      "loss:   0.7896937638521194\n",
      "loss:   0.7675802499055863\n",
      "loss:   0.7803879618644715\n",
      "loss:   0.7868599534034729\n",
      "loss:   0.7905747100710869\n",
      "loss:   0.7688040927052497\n",
      "loss:   0.7731907069683075\n",
      "loss:   0.7533746838569642\n",
      "loss:   0.7612786576151848\n",
      "loss:   0.7855290055274964\n",
      "loss:   0.7398007690906525\n",
      "loss:   0.7705547362565994\n",
      "loss:   0.74383524954319\n",
      "loss:   0.7444781467318535\n",
      "loss:   0.7622332885861397\n",
      "loss:   0.7400803416967392\n",
      "epoch: 10\n",
      "loss:   0.018636544048786164\n",
      "loss:   0.7743592441082001\n",
      "loss:   0.7263936869800091\n",
      "loss:   0.7308829426765442\n",
      "loss:   0.7514152556657792\n",
      "loss:   0.7259722769260406\n",
      "loss:   0.747178065776825\n",
      "loss:   0.7259611263871193\n",
      "loss:   0.7298229932785034\n",
      "loss:   0.7363749593496323\n",
      "loss:   0.741958849132061\n",
      "loss:   0.7437473118305207\n",
      "loss:   0.7307357430458069\n",
      "loss:   0.7119556203484535\n",
      "loss:   0.7117606550455093\n",
      "loss:   0.7244962990283966\n",
      "loss:   0.7457113116979599\n",
      "loss:   0.6844462364912033\n",
      "loss:   0.7159159511327744\n",
      "loss:   0.7190642103552818\n",
      "loss:   0.7331029675900936\n",
      "loss:   0.7250649064779282\n",
      "loss:   0.7077999487519264\n",
      "loss:   0.7069440633058548\n",
      "epoch: 11\n",
      "loss:   0.012304858863353729\n",
      "loss:   0.6931471318006516\n",
      "loss:   0.6924529664218426\n",
      "loss:   0.702184297144413\n",
      "loss:   0.713695977628231\n",
      "loss:   0.7140948973596096\n",
      "loss:   0.6989315152168274\n",
      "loss:   0.7193329468369484\n",
      "loss:   0.6688731037080288\n",
      "loss:   0.6935541525483131\n",
      "loss:   0.676550130546093\n",
      "loss:   0.6807201504707336\n",
      "loss:   0.7098555132746697\n",
      "loss:   0.6927226439118386\n",
      "loss:   0.6736721664667129\n",
      "loss:   0.6812823593616486\n",
      "loss:   0.6912530116736889\n",
      "loss:   0.691240006685257\n",
      "loss:   0.686307492852211\n",
      "loss:   0.675403679907322\n",
      "loss:   0.6968954205513\n",
      "loss:   0.6712903313338756\n",
      "loss:   0.6928754031658173\n",
      "loss:   0.6785093575716019\n",
      "epoch: 12\n",
      "loss:   0.015688255429267883\n",
      "loss:   0.660937675088644\n",
      "loss:   0.6651540890336036\n",
      "loss:   0.6665133662521839\n",
      "loss:   0.6762522175908089\n",
      "loss:   0.6554010361433029\n",
      "loss:   0.6883362621068955\n",
      "loss:   0.6448950044810772\n",
      "loss:   0.6741937942802906\n",
      "loss:   0.6609353080391884\n",
      "loss:   0.6396437257528305\n",
      "loss:   0.6566775098443032\n",
      "loss:   0.6743178978562355\n",
      "loss:   0.6400454744696618\n",
      "loss:   0.6537127010524273\n",
      "loss:   0.6566534548997879\n",
      "loss:   0.6297674596309661\n",
      "loss:   0.6719242073595524\n",
      "loss:   0.6684943810105324\n",
      "loss:   0.6579917348921299\n",
      "loss:   0.6596994757652282\n",
      "loss:   0.6600758410990238\n",
      "loss:   0.6376693390309811\n",
      "loss:   0.6521481603384018\n",
      "epoch: 13\n",
      "loss:   0.01592061370611191\n",
      "loss:   0.6120941542088986\n",
      "loss:   0.6595408320426941\n",
      "loss:   0.6138718605041504\n",
      "loss:   0.6650711201131344\n",
      "loss:   0.6533314973115921\n",
      "loss:   0.6548361673951149\n",
      "loss:   0.6427548132836819\n",
      "loss:   0.648026355355978\n",
      "loss:   0.632431723177433\n",
      "loss:   0.623254268616438\n",
      "loss:   0.6430149927735329\n",
      "loss:   0.6549299485981465\n",
      "loss:   0.6374320648610592\n",
      "loss:   0.6152058407664299\n",
      "loss:   0.6213261760771275\n",
      "loss:   0.6371181473135948\n",
      "loss:   0.6191503785550594\n",
      "loss:   0.5988506115972996\n",
      "loss:   0.6144403412938118\n",
      "loss:   0.6071166321635246\n",
      "loss:   0.6198040023446083\n",
      "loss:   0.6126444853842259\n",
      "loss:   0.6136637076735496\n",
      "epoch: 14\n",
      "loss:   0.01609831154346466\n",
      "loss:   0.6333504758775235\n",
      "loss:   0.6171217404305935\n",
      "loss:   0.6219064921140671\n",
      "loss:   0.645948538184166\n",
      "loss:   0.6105433538556099\n",
      "loss:   0.612857599556446\n",
      "loss:   0.5958005540072918\n",
      "loss:   0.6001607075333595\n",
      "loss:   0.5967975504696369\n",
      "loss:   0.5757264912128448\n",
      "loss:   0.6028941899538041\n",
      "loss:   0.6021712772548199\n",
      "loss:   0.6073076136410236\n",
      "loss:   0.5936272002756595\n",
      "loss:   0.5917225368320942\n",
      "loss:   0.5971196912229061\n",
      "loss:   0.6050348803400993\n",
      "loss:   0.6056422919034958\n",
      "loss:   0.604821077734232\n",
      "loss:   0.5870665490627289\n",
      "loss:   0.5722816303372383\n",
      "loss:   0.6182849325239659\n",
      "loss:   0.5995816968381404\n",
      "epoch: 15\n",
      "loss:   0.01817699372768402\n",
      "loss:   0.5989679813385009\n",
      "loss:   0.602644944190979\n",
      "loss:   0.58850042745471\n",
      "loss:   0.5924927741289139\n",
      "loss:   0.5972368009388447\n",
      "loss:   0.590240703523159\n",
      "loss:   0.5790406800806522\n",
      "loss:   0.5748757645487785\n",
      "loss:   0.5929873429238797\n",
      "loss:   0.584506269544363\n",
      "loss:   0.567920359224081\n",
      "loss:   0.5820775218307972\n",
      "loss:   0.5887503877282143\n",
      "loss:   0.5571338124573231\n",
      "loss:   0.5989139348268508\n",
      "loss:   0.5874930329620838\n",
      "loss:   0.5857658602297307\n",
      "loss:   0.6035405620932579\n",
      "loss:   0.5869965560734272\n",
      "loss:   0.5405543617904186\n",
      "loss:   0.5408175118267536\n",
      "loss:   0.5651200711727142\n",
      "loss:   0.5819213561713695\n",
      "epoch: 16\n",
      "loss:   0.014541278779506683\n",
      "loss:   0.5720536060631275\n",
      "loss:   0.5869806997478009\n",
      "loss:   0.5562018983066082\n",
      "loss:   0.5822679288685322\n",
      "loss:   0.5801736645400524\n",
      "loss:   0.5988558292388916\n",
      "loss:   0.5655704744160175\n",
      "loss:   0.5760902293026448\n",
      "loss:   0.559965131431818\n",
      "loss:   0.5535750634968281\n",
      "loss:   0.5611706845462322\n",
      "loss:   0.5571917004883289\n",
      "loss:   0.5796785041689873\n",
      "loss:   0.5299260094761848\n",
      "loss:   0.5543339811265469\n",
      "loss:   0.5573812164366245\n",
      "loss:   0.557215666770935\n",
      "loss:   0.5845573261380196\n",
      "loss:   0.5593581549823284\n",
      "loss:   0.5358352191746235\n",
      "loss:   0.5376480527222156\n",
      "loss:   0.5465590186417103\n",
      "loss:   0.5386464104056359\n",
      "epoch: 17\n",
      "loss:   0.01511521339416504\n",
      "loss:   0.545289809256792\n",
      "loss:   0.5580299809575081\n",
      "loss:   0.5544125720858574\n",
      "loss:   0.5417267568409443\n",
      "loss:   0.5358244180679321\n",
      "loss:   0.5543439008295536\n",
      "loss:   0.5401830643415451\n",
      "loss:   0.5382757231593132\n",
      "loss:   0.568513210862875\n",
      "loss:   0.5734203189611435\n",
      "loss:   0.5537443496286869\n",
      "loss:   0.5381561644375324\n",
      "loss:   0.5428104475140572\n",
      "loss:   0.5453509718179703\n",
      "loss:   0.5182659648358822\n",
      "loss:   0.5520338863134384\n",
      "loss:   0.5377930730581284\n",
      "loss:   0.5232922114431858\n",
      "loss:   0.5393707908689975\n",
      "loss:   0.5350669719278812\n",
      "loss:   0.5501698426902294\n",
      "loss:   0.5195224456489086\n",
      "loss:   0.5420331247150898\n",
      "epoch: 18\n",
      "loss:   0.01479332596063614\n",
      "loss:   0.5580607615411282\n",
      "loss:   0.5545802526175976\n",
      "loss:   0.529313700646162\n",
      "loss:   0.5463607028126717\n",
      "loss:   0.5055841661989688\n",
      "loss:   0.5143164336681366\n",
      "loss:   0.5301047913730145\n",
      "loss:   0.5220728732645512\n",
      "loss:   0.5303805753588676\n",
      "loss:   0.5166114084422588\n",
      "loss:   0.5404338434338569\n",
      "loss:   0.5242647223174572\n",
      "loss:   0.5151865012943745\n",
      "loss:   0.5387931346893311\n",
      "loss:   0.49458818361163137\n",
      "loss:   0.5138030141592026\n",
      "loss:   0.49620803222060206\n",
      "loss:   0.5379331782460213\n",
      "loss:   0.5484177641570568\n",
      "loss:   0.5383605755865574\n",
      "loss:   0.5175656914710999\n",
      "loss:   0.5245404273271561\n",
      "loss:   0.5553347133100033\n",
      "epoch: 19\n",
      "loss:   0.01612869054079056\n",
      "loss:   0.5060016244649888\n",
      "loss:   0.5234403736889363\n",
      "loss:   0.524825056642294\n",
      "loss:   0.5270997487008572\n",
      "loss:   0.5097983658313752\n",
      "loss:   0.517497967928648\n",
      "loss:   0.5059060603380203\n",
      "loss:   0.5264651618897915\n",
      "loss:   0.5034767150878906\n",
      "loss:   0.5281771935522557\n",
      "loss:   0.4950154088437557\n",
      "loss:   0.520782507956028\n",
      "loss:   0.5209083788096904\n",
      "loss:   0.5205276980996132\n",
      "loss:   0.4818251058459282\n",
      "loss:   0.5009964741766453\n",
      "loss:   0.5087493345141411\n",
      "loss:   0.5078251764178277\n",
      "loss:   0.5278608113527298\n",
      "loss:   0.5080089673399926\n",
      "loss:   0.5269289493560791\n",
      "loss:   0.5343251071870327\n",
      "loss:   0.4822599358856678\n",
      "epoch: 20\n",
      "loss:   0.012594415247440338\n",
      "loss:   0.5042719319462776\n",
      "loss:   0.493755628913641\n",
      "loss:   0.4979654118418694\n",
      "loss:   0.5080768659710884\n",
      "loss:   0.49326925799250604\n",
      "loss:   0.5158025197684765\n",
      "loss:   0.5036985836923122\n",
      "loss:   0.5036098897457123\n",
      "loss:   0.5058613032102585\n",
      "loss:   0.5169325485825539\n",
      "loss:   0.489514647424221\n",
      "loss:   0.48433576971292497\n",
      "loss:   0.48602480441331863\n",
      "loss:   0.5067658133804798\n",
      "loss:   0.49863807335495947\n",
      "loss:   0.5064333833754062\n",
      "loss:   0.4931535072624683\n",
      "loss:   0.49303692802786825\n",
      "loss:   0.4850171163678169\n",
      "loss:   0.49835396856069564\n",
      "loss:   0.4851813271641731\n",
      "loss:   0.5122888468205928\n",
      "loss:   0.5213133201003075\n",
      "epoch: 21\n",
      "loss:   0.014301556348800658\n",
      "loss:   0.4915359981358051\n",
      "loss:   0.4888216622173786\n",
      "loss:   0.4823390766978264\n",
      "loss:   0.4619723051786423\n",
      "loss:   0.5117239460349083\n",
      "loss:   0.507773757725954\n",
      "loss:   0.500584551692009\n",
      "loss:   0.4917649418115616\n",
      "loss:   0.49561895057559013\n",
      "loss:   0.4726839914917946\n",
      "loss:   0.47550742104649546\n",
      "loss:   0.5055480346083641\n",
      "loss:   0.48658513054251673\n",
      "loss:   0.47537253648042677\n",
      "loss:   0.4831300497055054\n",
      "loss:   0.477337709069252\n",
      "loss:   0.5063938304781914\n",
      "loss:   0.5028038464486599\n",
      "loss:   0.4763894945383072\n",
      "loss:   0.4774244472384453\n",
      "loss:   0.48159167394042013\n",
      "loss:   0.4866755597293377\n",
      "loss:   0.5038243673741818\n",
      "epoch: 22\n",
      "loss:   0.00900903195142746\n",
      "loss:   0.4830323182046413\n",
      "loss:   0.48086132705211637\n",
      "loss:   0.48732192143797876\n",
      "loss:   0.46969047337770464\n",
      "loss:   0.49183007404208184\n",
      "loss:   0.4748110115528107\n",
      "loss:   0.45128685534000396\n",
      "loss:   0.4868138939142227\n",
      "loss:   0.46397511214017867\n",
      "loss:   0.4683247022330761\n",
      "loss:   0.4717299848794937\n",
      "loss:   0.47469593957066536\n",
      "loss:   0.4988051228225231\n",
      "loss:   0.48314878046512605\n",
      "loss:   0.4958911001682281\n",
      "loss:   0.47096012160182\n",
      "loss:   0.48673001155257223\n",
      "loss:   0.47482368126511576\n",
      "loss:   0.4311411716043949\n",
      "loss:   0.48663696721196176\n",
      "loss:   0.48123610094189645\n",
      "loss:   0.49476380571722983\n",
      "loss:   0.4856046162545681\n",
      "epoch: 23\n",
      "loss:   0.012899404764175415\n",
      "loss:   0.4757719159126282\n",
      "loss:   0.4921157002449036\n",
      "loss:   0.4922482565045357\n",
      "loss:   0.48148429244756696\n",
      "loss:   0.44790816158056257\n",
      "loss:   0.48078912794589995\n",
      "loss:   0.4810968145728111\n",
      "loss:   0.4900180913507938\n",
      "loss:   0.4811552047729492\n",
      "loss:   0.4727861776947975\n",
      "loss:   0.4684269778430462\n",
      "loss:   0.45769440159201624\n",
      "loss:   0.47928197160363195\n",
      "loss:   0.482842405885458\n",
      "loss:   0.4435419231653214\n",
      "loss:   0.4800277203321457\n",
      "loss:   0.4564519803971052\n",
      "loss:   0.4632625229656696\n",
      "loss:   0.4683707308024168\n",
      "loss:   0.4364866506308317\n",
      "loss:   0.45138224512338637\n",
      "loss:   0.44646159932017326\n",
      "loss:   0.45310407876968384\n",
      "epoch: 24\n",
      "loss:   0.015173448622226715\n",
      "loss:   0.4680075030773878\n",
      "loss:   0.46129973530769347\n",
      "loss:   0.4884975768625736\n",
      "loss:   0.44195192530751226\n",
      "loss:   0.439237891882658\n",
      "loss:   0.4267077695578337\n",
      "loss:   0.4357884705066681\n",
      "loss:   0.48945408388972284\n",
      "loss:   0.43155745044350624\n",
      "loss:   0.45926810801029205\n",
      "loss:   0.46749075353145597\n",
      "loss:   0.4544626548886299\n",
      "loss:   0.4647411987185478\n",
      "loss:   0.48132631927728653\n",
      "loss:   0.4921304456889629\n",
      "loss:   0.4594206131994724\n",
      "loss:   0.4968698784708977\n",
      "loss:   0.46533547565340994\n",
      "loss:   0.44202949330210684\n",
      "loss:   0.43309987224638463\n",
      "loss:   0.467807674407959\n",
      "loss:   0.46764953806996346\n",
      "loss:   0.445342929661274\n",
      "epoch: 25\n",
      "loss:   0.014871516823768615\n",
      "loss:   0.4737255282700062\n",
      "loss:   0.4543568331748247\n",
      "loss:   0.452187679708004\n",
      "loss:   0.4528237581253052\n",
      "loss:   0.46703642010688784\n",
      "loss:   0.43563075363636017\n",
      "loss:   0.4531473457813263\n",
      "loss:   0.43390100076794624\n",
      "loss:   0.4303277261555195\n",
      "loss:   0.4347499690949917\n",
      "loss:   0.44620596766471865\n",
      "loss:   0.47258550822734835\n",
      "loss:   0.43907529264688494\n",
      "loss:   0.4706500321626663\n",
      "loss:   0.44439947605133057\n",
      "loss:   0.4633427605032921\n",
      "loss:   0.42492727637290956\n",
      "loss:   0.45141594782471656\n",
      "loss:   0.4464095365256071\n",
      "loss:   0.46947271600365637\n",
      "loss:   0.4696546796709299\n",
      "loss:   0.45547822304069996\n",
      "loss:   0.4642358168959618\n",
      "epoch: 26\n",
      "loss:   0.009293346852064132\n",
      "loss:   0.441312437877059\n",
      "loss:   0.4595891945064068\n",
      "loss:   0.42932612001895903\n",
      "loss:   0.4257261618971825\n",
      "loss:   0.4526370707899332\n",
      "loss:   0.4330240271985531\n",
      "loss:   0.45352608896791935\n",
      "loss:   0.4570825845003128\n",
      "loss:   0.44382902085781095\n",
      "loss:   0.45096910297870635\n",
      "loss:   0.43504693880677225\n",
      "loss:   0.44254920408129694\n",
      "loss:   0.4340495217591524\n",
      "loss:   0.4427912212908268\n",
      "loss:   0.45047339498996736\n",
      "loss:   0.4458076559007168\n",
      "loss:   0.4401239659637213\n",
      "loss:   0.4282800883054733\n",
      "loss:   0.45005243048071863\n",
      "loss:   0.4491628482937813\n",
      "loss:   0.44813359454274176\n",
      "loss:   0.4697689387947321\n",
      "loss:   0.43223022744059564\n",
      "epoch: 27\n",
      "loss:   0.013673053681850433\n",
      "loss:   0.4464957773685455\n",
      "loss:   0.41712800972163677\n",
      "loss:   0.4380614813417196\n",
      "loss:   0.4331406868994236\n",
      "loss:   0.3985823165625334\n",
      "loss:   0.45284063555300236\n",
      "loss:   0.4221364725381136\n",
      "loss:   0.43025558441877365\n",
      "loss:   0.4403485290706158\n",
      "loss:   0.4470975182950497\n",
      "loss:   0.43721488639712336\n",
      "loss:   0.4249101150780916\n",
      "loss:   0.443258985131979\n",
      "loss:   0.44720253348350525\n",
      "loss:   0.4441987037658691\n",
      "loss:   0.45891249403357504\n",
      "loss:   0.43991117030382154\n",
      "loss:   0.4359351471066475\n",
      "loss:   0.4308247245848179\n",
      "loss:   0.44872905910015104\n",
      "loss:   0.41805751286447046\n",
      "loss:   0.41258798018097875\n",
      "loss:   0.47589538171887397\n",
      "epoch: 28\n",
      "loss:   0.006716185808181762\n",
      "loss:   0.4534584734588861\n",
      "loss:   0.41366415470838547\n",
      "loss:   0.4439488686621189\n",
      "loss:   0.42830339036881926\n",
      "loss:   0.4253924280405045\n",
      "loss:   0.40663034543395044\n",
      "loss:   0.4510080263018608\n",
      "loss:   0.4505202278494835\n",
      "loss:   0.4481715328991413\n",
      "loss:   0.4428408868610859\n",
      "loss:   0.42748942226171494\n",
      "loss:   0.44289565533399583\n",
      "loss:   0.41801164112985134\n",
      "loss:   0.4341411184519529\n",
      "loss:   0.4346694976091385\n",
      "loss:   0.42277262955904005\n",
      "loss:   0.4308351829648018\n",
      "loss:   0.43257438801229\n",
      "loss:   0.4103005867451429\n",
      "loss:   0.41932354047894477\n",
      "loss:   0.4261808045208454\n",
      "loss:   0.44121736884117124\n",
      "loss:   0.43435791730880735\n",
      "epoch: 29\n",
      "loss:   0.01091645210981369\n",
      "loss:   0.41384799778461456\n",
      "loss:   0.426177691668272\n",
      "loss:   0.42824737504124644\n",
      "loss:   0.44883465617895124\n",
      "loss:   0.4412708945572376\n",
      "loss:   0.4152830883860588\n",
      "loss:   0.4070924919098616\n",
      "loss:   0.4345187596976757\n",
      "loss:   0.44720011204481125\n",
      "loss:   0.42863981872797013\n",
      "loss:   0.4384657755494118\n",
      "loss:   0.42476699128746986\n",
      "loss:   0.4108086831867695\n",
      "loss:   0.4271246075630188\n",
      "loss:   0.4150890804827213\n",
      "loss:   0.40774171650409696\n",
      "loss:   0.4535192534327507\n",
      "loss:   0.41407373882830145\n",
      "loss:   0.414341526851058\n",
      "loss:   0.4293603353202343\n",
      "loss:   0.4177713967859745\n",
      "loss:   0.42984798178076744\n",
      "loss:   0.4197017215192318\n",
      "epoch: 30\n",
      "loss:   0.0113642580807209\n",
      "loss:   0.4185915134847164\n",
      "loss:   0.42584225311875346\n",
      "loss:   0.4578325718641281\n",
      "loss:   0.42253023348748686\n",
      "loss:   0.40674711726605894\n",
      "loss:   0.42220706045627593\n",
      "loss:   0.42061725705862046\n",
      "loss:   0.4016863241791725\n",
      "loss:   0.41165815070271494\n",
      "loss:   0.4239918738603592\n",
      "loss:   0.40552485585212705\n",
      "loss:   0.4423880398273468\n",
      "loss:   0.410409452393651\n",
      "loss:   0.40179123282432555\n",
      "loss:   0.43189655654132364\n",
      "loss:   0.4254950888454914\n",
      "loss:   0.4316922347992659\n",
      "loss:   0.409335570782423\n",
      "loss:   0.4196124766021967\n",
      "loss:   0.45027183964848516\n",
      "loss:   0.41374256908893586\n",
      "loss:   0.41363391652703285\n",
      "loss:   0.41076287180185317\n",
      "epoch: 31\n",
      "loss:   0.010439208894968032\n",
      "loss:   0.40722754672169686\n",
      "loss:   0.414272128790617\n",
      "loss:   0.4103778827935457\n",
      "loss:   0.4336686495691538\n",
      "loss:   0.40988822020590304\n",
      "loss:   0.4060461435467005\n",
      "loss:   0.42420811094343663\n",
      "loss:   0.3974178690463305\n",
      "loss:   0.43341380059719087\n",
      "loss:   0.40700413659214973\n",
      "loss:   0.428258566185832\n",
      "loss:   0.3785796411335468\n",
      "loss:   0.4317913204431534\n",
      "loss:   0.44766981191933153\n",
      "loss:   0.4001780319958925\n",
      "loss:   0.42081967890262606\n",
      "loss:   0.4176107529550791\n",
      "loss:   0.4013819195330143\n",
      "loss:   0.42672887332737447\n",
      "loss:   0.39692673087120056\n",
      "loss:   0.4462635193020105\n",
      "loss:   0.40514931492507456\n",
      "loss:   0.410330431163311\n",
      "epoch: 32\n",
      "loss:   0.0077575728297233585\n",
      "loss:   0.390258976444602\n",
      "loss:   0.39972041808068753\n",
      "loss:   0.40757915675640105\n",
      "loss:   0.42119255363941194\n",
      "loss:   0.407429501786828\n",
      "loss:   0.38457983881235125\n",
      "loss:   0.409837094694376\n",
      "loss:   0.44194629453122614\n",
      "loss:   0.4103244923055172\n",
      "loss:   0.4231965720653534\n",
      "loss:   0.4143380083143711\n",
      "loss:   0.4378507468849421\n",
      "loss:   0.4004199389368296\n",
      "loss:   0.43597940132021906\n",
      "loss:   0.3997297067195177\n",
      "loss:   0.43048315197229386\n",
      "loss:   0.40591024309396745\n",
      "loss:   0.4054520808160305\n",
      "loss:   0.3742956452071667\n",
      "loss:   0.42022068947553637\n",
      "loss:   0.4187039643526077\n",
      "loss:   0.3617764487862587\n",
      "loss:   0.415409966185689\n",
      "epoch: 33\n",
      "loss:   0.01185273304581642\n",
      "loss:   0.429591104760766\n",
      "loss:   0.4294593833386898\n",
      "loss:   0.4050999753177166\n",
      "loss:   0.4036290027201176\n",
      "loss:   0.4007869653403759\n",
      "loss:   0.4402252037078142\n",
      "loss:   0.3683072619140148\n",
      "loss:   0.41130726374685767\n",
      "loss:   0.4111858431249857\n",
      "loss:   0.4239395458251238\n",
      "loss:   0.4134317319840193\n",
      "loss:   0.3521324597299099\n",
      "loss:   0.41215430982410906\n",
      "loss:   0.3886254861950874\n",
      "loss:   0.4235541097819805\n",
      "loss:   0.3709975887089968\n",
      "loss:   0.42917827926576135\n",
      "loss:   0.3886631719768047\n",
      "loss:   0.4156382463872433\n",
      "loss:   0.4161167610436678\n",
      "loss:   0.3902196686714888\n",
      "loss:   0.3997502960264683\n",
      "loss:   0.4142636679112911\n",
      "epoch: 34\n",
      "loss:   0.0063047058880329136\n",
      "loss:   0.38886627182364464\n",
      "loss:   0.4023475658148527\n",
      "loss:   0.40043031647801397\n",
      "loss:   0.39072305262088775\n",
      "loss:   0.4075366102159023\n",
      "loss:   0.39727346003055575\n",
      "loss:   0.38925523348152635\n",
      "loss:   0.39947762824594973\n",
      "loss:   0.4120261870324612\n",
      "loss:   0.4306198686361313\n",
      "loss:   0.41549262404441833\n",
      "loss:   0.3763515200465918\n",
      "loss:   0.41015358492732046\n",
      "loss:   0.4141621869057417\n",
      "loss:   0.3892078962177038\n",
      "loss:   0.4165069177746773\n",
      "loss:   0.40135369673371313\n",
      "loss:   0.4071660920977592\n",
      "loss:   0.40014813393354415\n",
      "loss:   0.4150617182254791\n",
      "loss:   0.4057533510029316\n",
      "loss:   0.4177906159311533\n",
      "loss:   0.37236678414046764\n",
      "epoch: 35\n",
      "loss:   0.009108936786651612\n",
      "loss:   0.42997336611151693\n",
      "loss:   0.37437269538640977\n",
      "loss:   0.3891433998942375\n",
      "loss:   0.38095361478626727\n",
      "loss:   0.3972406078130007\n",
      "loss:   0.4309020332992077\n",
      "loss:   0.4006211802363396\n",
      "loss:   0.4007442984730005\n",
      "loss:   0.39506984651088717\n",
      "loss:   0.3895693551748991\n",
      "loss:   0.40463444255292413\n",
      "loss:   0.43335572332143785\n",
      "loss:   0.3878103282302618\n",
      "loss:   0.40191134437918663\n",
      "loss:   0.3715369436889887\n",
      "loss:   0.4321034915745258\n",
      "loss:   0.38273707032203674\n",
      "loss:   0.37310250848531723\n",
      "loss:   0.38114664554595945\n",
      "loss:   0.3972533494234085\n",
      "loss:   0.40894157700240613\n",
      "loss:   0.4008302312344313\n",
      "loss:   0.37449690513312817\n",
      "epoch: 36\n",
      "loss:   0.012358113378286361\n",
      "loss:   0.39063757956027984\n",
      "loss:   0.4185702450573444\n",
      "loss:   0.3960368439555168\n",
      "loss:   0.3939768474549055\n",
      "loss:   0.39353862442076204\n",
      "loss:   0.39151981733739377\n",
      "loss:   0.3858900718390942\n",
      "loss:   0.40260629914700985\n",
      "loss:   0.3677158188074827\n",
      "loss:   0.3831320356577635\n",
      "loss:   0.4052536800503731\n",
      "loss:   0.3854115601629019\n",
      "loss:   0.4040547862648964\n",
      "loss:   0.3932874582707882\n",
      "loss:   0.3754917852580547\n",
      "loss:   0.4137117188423872\n",
      "loss:   0.3806080732494593\n",
      "loss:   0.4253659337759018\n",
      "loss:   0.358265233412385\n",
      "loss:   0.43122086152434347\n",
      "loss:   0.3937185287475586\n",
      "loss:   0.39270335957407954\n",
      "loss:   0.36788990423083306\n",
      "epoch: 37\n",
      "loss:   0.009115153551101684\n",
      "loss:   0.3827511828392744\n",
      "loss:   0.39056498259305955\n",
      "loss:   0.36888517774641516\n",
      "loss:   0.4034667547792196\n",
      "loss:   0.3983131110668182\n",
      "loss:   0.34400544241070746\n",
      "loss:   0.40809468254446984\n",
      "loss:   0.430990531668067\n",
      "loss:   0.40992207303643224\n",
      "loss:   0.3767591163516045\n",
      "loss:   0.38555677644908426\n",
      "loss:   0.40467594340443613\n",
      "loss:   0.3708171956241131\n",
      "loss:   0.40018707513809204\n",
      "loss:   0.3860010117292404\n",
      "loss:   0.3869015332311392\n",
      "loss:   0.3976850416511297\n",
      "loss:   0.37096774876117705\n",
      "loss:   0.3890122402459383\n",
      "loss:   0.38011996895074845\n",
      "loss:   0.40113442838191987\n",
      "loss:   0.38092894554138185\n",
      "loss:   0.38341692835092545\n",
      "epoch: 38\n",
      "loss:   0.010648452490568162\n",
      "loss:   0.38986528292298317\n",
      "loss:   0.3901520572602749\n",
      "loss:   0.41197189278900626\n",
      "loss:   0.3883938927203417\n",
      "loss:   0.3826401084661484\n",
      "loss:   0.36840578988194467\n",
      "loss:   0.39215657114982605\n",
      "loss:   0.4102170065045357\n",
      "loss:   0.3672355808317661\n",
      "loss:   0.39295235760509967\n",
      "loss:   0.3902430761605501\n",
      "loss:   0.377241812646389\n",
      "loss:   0.382949373871088\n",
      "loss:   0.379322449490428\n",
      "loss:   0.3644250124692917\n",
      "loss:   0.4002534657716751\n",
      "loss:   0.37339096441864966\n",
      "loss:   0.35536084286868574\n",
      "loss:   0.4339881677180529\n",
      "loss:   0.39169471748173235\n",
      "loss:   0.3717016614973545\n",
      "loss:   0.37247745394706727\n",
      "loss:   0.4162972241640091\n",
      "epoch: 39\n",
      "loss:   0.01162799671292305\n",
      "loss:   0.35590460114181044\n",
      "loss:   0.37847123220562934\n",
      "loss:   0.38854031153023244\n",
      "loss:   0.3818311970680952\n",
      "loss:   0.3881127335131168\n",
      "loss:   0.36823001205921174\n",
      "loss:   0.3969783719629049\n",
      "loss:   0.3649274975061417\n",
      "loss:   0.41305366829037665\n",
      "loss:   0.38514586314558985\n",
      "loss:   0.379360968619585\n",
      "loss:   0.40252282582223414\n",
      "loss:   0.3825040068477392\n",
      "loss:   0.3778631776571274\n",
      "loss:   0.38156106770038606\n",
      "loss:   0.37875081934034827\n",
      "loss:   0.3949810560792685\n",
      "loss:   0.3749271538108587\n",
      "loss:   0.37386396825313567\n",
      "loss:   0.3761385530233383\n",
      "loss:   0.37246826440095904\n",
      "loss:   0.4027979180216789\n",
      "loss:   0.400870369002223\n",
      "epoch: 40\n",
      "loss:   0.012779557704925537\n",
      "loss:   0.3470302473753691\n",
      "loss:   0.37071785554289816\n",
      "loss:   0.399647793918848\n",
      "loss:   0.3861392877995968\n",
      "loss:   0.39981034845113755\n",
      "loss:   0.35023229718208315\n",
      "loss:   0.3758730970323086\n",
      "loss:   0.37357690297067164\n",
      "loss:   0.3942799441516399\n",
      "loss:   0.37232238315045835\n",
      "loss:   0.3815197002142668\n",
      "loss:   0.3914514172822237\n",
      "loss:   0.4123952142894268\n",
      "loss:   0.38370954543352126\n",
      "loss:   0.3643490795046091\n",
      "loss:   0.39454200156033037\n",
      "loss:   0.3625784069299698\n",
      "loss:   0.39089119136333467\n",
      "loss:   0.35950107127428055\n",
      "loss:   0.3600487135350704\n",
      "loss:   0.37631427198648454\n",
      "loss:   0.37702180445194244\n",
      "loss:   0.4113303869962692\n",
      "epoch: 41\n",
      "loss:   0.005619540810585022\n",
      "loss:   0.3695548553019762\n",
      "loss:   0.35848848335444927\n",
      "loss:   0.3812267206609249\n",
      "loss:   0.37584702521562574\n",
      "loss:   0.38473112806677817\n",
      "loss:   0.40743265114724636\n",
      "loss:   0.36477172300219535\n",
      "loss:   0.36892112977802755\n",
      "loss:   0.36223677210509775\n",
      "loss:   0.3901094049215317\n",
      "loss:   0.36023314781486987\n",
      "loss:   0.3674627423286438\n",
      "loss:   0.36368729881942274\n",
      "loss:   0.3960221793502569\n",
      "loss:   0.35823732390999796\n",
      "loss:   0.3825377706438303\n",
      "loss:   0.3697183825075626\n",
      "loss:   0.37966056540608406\n",
      "loss:   0.3832689870148897\n",
      "loss:   0.35780507512390614\n",
      "loss:   0.43284331262111664\n",
      "loss:   0.39750549755990505\n",
      "loss:   0.37456986829638483\n",
      "epoch: 42\n",
      "loss:   0.008962836116552353\n",
      "loss:   0.3648966498672962\n",
      "loss:   0.41383795738220214\n",
      "loss:   0.3671346314251423\n",
      "loss:   0.3595360066741705\n",
      "loss:   0.34579072520136833\n",
      "loss:   0.387316557392478\n",
      "loss:   0.34836599715054034\n",
      "loss:   0.37453536726534364\n",
      "loss:   0.3908053085207939\n",
      "loss:   0.33996194042265415\n",
      "loss:   0.4000155314803123\n",
      "loss:   0.36963686160743237\n",
      "loss:   0.3583824336528778\n",
      "loss:   0.4139746014028788\n",
      "loss:   0.3810792498290539\n",
      "loss:   0.37048750519752505\n",
      "loss:   0.38843424171209334\n",
      "loss:   0.3704180262982845\n",
      "loss:   0.4057539813220501\n",
      "loss:   0.32485181726515294\n",
      "loss:   0.37149796821177006\n",
      "loss:   0.38941681683063506\n",
      "loss:   0.3782102666795254\n",
      "epoch: 43\n",
      "loss:   0.009464721381664275\n",
      "loss:   0.37412268221378325\n",
      "loss:   0.357745498418808\n",
      "loss:   0.3920643083751202\n",
      "loss:   0.3630821518599987\n",
      "loss:   0.3581535004079342\n",
      "loss:   0.34493253789842127\n",
      "loss:   0.3561912581324577\n",
      "loss:   0.3750834222882986\n",
      "loss:   0.3953722920268774\n",
      "loss:   0.36026895232498646\n",
      "loss:   0.34683287180960176\n",
      "loss:   0.38042840659618377\n",
      "loss:   0.37659432403743265\n",
      "loss:   0.40271909460425376\n",
      "loss:   0.3879278123378754\n",
      "loss:   0.3596992377191782\n",
      "loss:   0.3899125061929226\n",
      "loss:   0.34605448357760904\n",
      "loss:   0.3903746780008078\n",
      "loss:   0.3610097263008356\n",
      "loss:   0.37321569956839085\n",
      "loss:   0.37863822355866433\n",
      "loss:   0.37374779880046843\n",
      "epoch: 44\n",
      "loss:   0.008474544435739518\n",
      "loss:   0.35733251832425594\n",
      "loss:   0.369934056699276\n",
      "loss:   0.3697263214737177\n",
      "loss:   0.3938557580113411\n",
      "loss:   0.3394222937524319\n",
      "loss:   0.3647795166820288\n",
      "loss:   0.3559463646262884\n",
      "loss:   0.3575520358979702\n",
      "loss:   0.36774381399154665\n",
      "loss:   0.41649723798036575\n",
      "loss:   0.3649151649326086\n",
      "loss:   0.362388376891613\n",
      "loss:   0.3755176905542612\n",
      "loss:   0.37636973708868027\n",
      "loss:   0.3823698125779629\n",
      "loss:   0.38798592016100886\n",
      "loss:   0.3606234256178141\n",
      "loss:   0.37377706430852414\n",
      "loss:   0.32769777737557887\n",
      "loss:   0.37969641759991646\n",
      "loss:   0.3652297183871269\n",
      "loss:   0.36075491458177567\n",
      "loss:   0.36776878908276556\n",
      "epoch: 45\n",
      "loss:   0.006123806908726692\n",
      "loss:   0.3611988712102175\n",
      "loss:   0.3793609868735075\n",
      "loss:   0.35533770006150006\n",
      "loss:   0.3755184579640627\n",
      "loss:   0.37445811107754706\n",
      "loss:   0.3652392327785492\n",
      "loss:   0.3768883842974901\n",
      "loss:   0.3572619318962097\n",
      "loss:   0.3703963741660118\n",
      "loss:   0.33997769467532635\n",
      "loss:   0.38571761138737204\n",
      "loss:   0.3775508351624012\n",
      "loss:   0.3621538314968348\n",
      "loss:   0.373625160753727\n",
      "loss:   0.39542482756078245\n",
      "loss:   0.3624156128615141\n",
      "loss:   0.3619566362351179\n",
      "loss:   0.3448893465101719\n",
      "loss:   0.3545029677450657\n",
      "loss:   0.352626058831811\n",
      "loss:   0.3502805933356285\n",
      "loss:   0.3573014959692955\n",
      "loss:   0.39832445830106733\n",
      "epoch: 46\n",
      "loss:   0.010215569287538528\n",
      "loss:   0.3613963101059198\n",
      "loss:   0.3711501810699701\n",
      "loss:   0.3958615481853485\n",
      "loss:   0.38145384602248666\n",
      "loss:   0.3588504157960415\n",
      "loss:   0.35151704475283624\n",
      "loss:   0.3874250866472721\n",
      "loss:   0.3661575749516487\n",
      "loss:   0.3782985437661409\n",
      "loss:   0.34477361738681794\n",
      "loss:   0.3425627749413252\n",
      "loss:   0.35436651222407817\n",
      "loss:   0.3805616568773985\n",
      "loss:   0.3691524140536785\n",
      "loss:   0.3696783758699894\n",
      "loss:   0.3712050288915634\n",
      "loss:   0.3593286741524935\n",
      "loss:   0.36016000248491764\n",
      "loss:   0.39265187866985796\n",
      "loss:   0.36120425499975684\n",
      "loss:   0.34997324347496034\n",
      "loss:   0.35419896915555\n",
      "loss:   0.32141272015869615\n",
      "epoch: 47\n",
      "loss:   0.010589008033275605\n",
      "loss:   0.3705920740962029\n",
      "loss:   0.3867289889603853\n",
      "loss:   0.37142767682671546\n",
      "loss:   0.36236316077411174\n",
      "loss:   0.3388172395527363\n",
      "loss:   0.3367709755897522\n",
      "loss:   0.355679078400135\n",
      "loss:   0.36013617031276224\n",
      "loss:   0.37148025780916216\n",
      "loss:   0.39228947944939135\n",
      "loss:   0.33893068730831144\n",
      "loss:   0.34599613323807715\n",
      "loss:   0.3786957014352083\n",
      "loss:   0.3538875702768564\n",
      "loss:   0.3602351900190115\n",
      "loss:   0.37548371851444245\n",
      "loss:   0.38866414912045\n",
      "loss:   0.34949262961745264\n",
      "loss:   0.39720994755625727\n",
      "loss:   0.37832481861114503\n",
      "loss:   0.3490747928619385\n",
      "loss:   0.3250086821615696\n",
      "loss:   0.34527429975569246\n",
      "epoch: 48\n",
      "loss:   0.01646343469619751\n",
      "loss:   0.3492645755410194\n",
      "loss:   0.36696798242628575\n",
      "loss:   0.37392533496022223\n",
      "loss:   0.35089904740452765\n",
      "loss:   0.35909172892570496\n",
      "loss:   0.34383675940334796\n",
      "loss:   0.3609890569001436\n",
      "loss:   0.35615960098803046\n",
      "loss:   0.3444035332649946\n",
      "loss:   0.3390725050121546\n",
      "loss:   0.3478178933262825\n",
      "loss:   0.39972422309219835\n",
      "loss:   0.3668677441775799\n",
      "loss:   0.3456443756818771\n",
      "loss:   0.3600489482283592\n",
      "loss:   0.3586208337917924\n",
      "loss:   0.34928868263959884\n",
      "loss:   0.3515974199399352\n",
      "loss:   0.3782065574079752\n",
      "loss:   0.3793927598744631\n",
      "loss:   0.35462788231670855\n",
      "loss:   0.3532599855214357\n",
      "loss:   0.3722677856683731\n",
      "epoch: 49\n",
      "loss:   0.009676624089479446\n",
      "loss:   0.3505450192838907\n",
      "loss:   0.36776227690279484\n",
      "loss:   0.34580625034868717\n",
      "loss:   0.3368364334106445\n",
      "loss:   0.3514225374907255\n",
      "loss:   0.3319005183875561\n",
      "loss:   0.3453911505639553\n",
      "loss:   0.3522219769656658\n",
      "loss:   0.35175758637487886\n",
      "loss:   0.3428546618670225\n",
      "loss:   0.3330972332507372\n",
      "loss:   0.3768285818397999\n",
      "loss:   0.35289682596921923\n",
      "loss:   0.37346296906471255\n",
      "loss:   0.40418269000947477\n",
      "loss:   0.35149008594453335\n",
      "loss:   0.3778038345277309\n",
      "loss:   0.362235389277339\n",
      "loss:   0.38094787299633026\n",
      "loss:   0.3876502301543951\n",
      "loss:   0.3494329541921616\n",
      "loss:   0.3406558971852064\n",
      "loss:   0.3519995901733637\n",
      "epoch: 50\n",
      "loss:   0.01133524551987648\n",
      "loss:   0.3477241888642311\n",
      "loss:   0.3412809453904629\n",
      "loss:   0.3658740121871233\n",
      "loss:   0.3449590466916561\n",
      "loss:   0.3570428237318993\n",
      "loss:   0.3570131231099367\n",
      "loss:   0.3616210389882326\n",
      "loss:   0.36658661589026453\n",
      "loss:   0.34227941036224363\n",
      "loss:   0.34838198944926263\n",
      "loss:   0.3721138909459114\n",
      "loss:   0.35607554353773596\n",
      "loss:   0.3678010258823633\n",
      "loss:   0.3435810815542936\n",
      "loss:   0.37802737206220627\n",
      "loss:   0.3704830467700958\n",
      "loss:   0.35638256296515464\n",
      "loss:   0.34392213001847266\n",
      "loss:   0.3739682585000992\n",
      "loss:   0.33117001354694364\n",
      "loss:   0.3280187167227268\n",
      "loss:   0.346093625202775\n",
      "loss:   0.3450626853853464\n",
      "epoch: 51\n",
      "loss:   0.009050172567367554\n",
      "loss:   0.33051766119897363\n",
      "loss:   0.3880801860243082\n",
      "loss:   0.34912616312503814\n",
      "loss:   0.3577940609306097\n",
      "loss:   0.32640050575137136\n",
      "loss:   0.35617731772363187\n",
      "loss:   0.36787531338632107\n",
      "loss:   0.33844572119414806\n",
      "loss:   0.3490544956177473\n",
      "loss:   0.3586019199341536\n",
      "loss:   0.37362871393561364\n",
      "loss:   0.3482052594423294\n",
      "loss:   0.34140738695859907\n",
      "loss:   0.3500264223664999\n",
      "loss:   0.33712935782969\n",
      "loss:   0.37454709969460964\n",
      "loss:   0.3327170735225081\n",
      "loss:   0.3404674168676138\n",
      "loss:   0.32880403101444244\n",
      "loss:   0.37878523357212546\n",
      "loss:   0.34399590492248533\n",
      "loss:   0.35186523385345936\n",
      "loss:   0.37821499407291415\n",
      "epoch: 52\n",
      "loss:   0.009609387814998626\n",
      "loss:   0.34380775727331636\n",
      "loss:   0.31792058385908606\n",
      "loss:   0.34047313407063484\n",
      "loss:   0.3421971250325441\n",
      "loss:   0.36173152960836885\n",
      "loss:   0.34746447429060934\n",
      "loss:   0.36826178058981895\n",
      "loss:   0.39080095328390596\n",
      "loss:   0.3320144645869732\n",
      "loss:   0.3420212961733341\n",
      "loss:   0.3688563100993633\n",
      "loss:   0.3586554419249296\n",
      "loss:   0.3683702234178782\n",
      "loss:   0.33616026006639005\n",
      "loss:   0.36528717130422594\n",
      "loss:   0.31466652899980546\n",
      "loss:   0.350731759890914\n",
      "loss:   0.3611737720668316\n",
      "loss:   0.33300302661955355\n",
      "loss:   0.34998235292732716\n",
      "loss:   0.37151295505464077\n",
      "loss:   0.3585438381880522\n",
      "loss:   0.3551603652536869\n",
      "epoch: 53\n",
      "loss:   0.004511899128556251\n",
      "loss:   0.32228247858583925\n",
      "loss:   0.3443905126303434\n",
      "loss:   0.3798979535698891\n",
      "loss:   0.3423034906387329\n",
      "loss:   0.36230736821889875\n",
      "loss:   0.35827840529382227\n",
      "loss:   0.3734037060290575\n",
      "loss:   0.3590151324868202\n",
      "loss:   0.3413779206573963\n",
      "loss:   0.3492283415049314\n",
      "loss:   0.3178974959999323\n",
      "loss:   0.34736771248281\n",
      "loss:   0.36049417220056057\n",
      "loss:   0.32632048800587654\n",
      "loss:   0.35442530550062656\n",
      "loss:   0.32909913137555125\n",
      "loss:   0.34913882687687875\n",
      "loss:   0.3612240504473448\n",
      "loss:   0.3637257568538189\n",
      "loss:   0.33854198530316354\n",
      "loss:   0.339324139803648\n",
      "loss:   0.33729580976068974\n",
      "loss:   0.36489111445844175\n",
      "epoch: 54\n",
      "loss:   0.0126027911901474\n",
      "loss:   0.33565046153962613\n",
      "loss:   0.36986616551876067\n",
      "loss:   0.3448319133371115\n",
      "loss:   0.35408969298005105\n",
      "loss:   0.3679415386170149\n",
      "loss:   0.325746563076973\n",
      "loss:   0.33927247673273087\n",
      "loss:   0.3419145464897156\n",
      "loss:   0.3604740113019943\n",
      "loss:   0.3234642710536718\n",
      "loss:   0.3523374466225505\n",
      "loss:   0.30868810750544073\n",
      "loss:   0.3775248173624277\n",
      "loss:   0.36075740717351434\n",
      "loss:   0.3423837684094906\n",
      "loss:   0.37444959841668607\n",
      "loss:   0.3449021641165018\n",
      "loss:   0.34560492523014547\n",
      "loss:   0.3453197285532951\n",
      "loss:   0.3373905885964632\n",
      "loss:   0.35831964798271654\n",
      "loss:   0.3404652062803507\n",
      "loss:   0.3270387854427099\n",
      "epoch: 55\n",
      "loss:   0.007395528256893158\n",
      "loss:   0.3575322352349758\n",
      "loss:   0.34447570852935316\n",
      "loss:   0.33143053464591504\n",
      "loss:   0.33378269262611865\n",
      "loss:   0.3550747338682413\n",
      "loss:   0.33851911425590514\n",
      "loss:   0.3474296510219574\n",
      "loss:   0.3394107323139906\n",
      "loss:   0.3451525166630745\n",
      "loss:   0.35366737879812715\n",
      "loss:   0.34884141758084297\n",
      "loss:   0.34541239216923714\n",
      "loss:   0.3214669533073902\n",
      "loss:   0.34285783022642136\n",
      "loss:   0.32595782428979875\n",
      "loss:   0.337287687510252\n",
      "loss:   0.36551749855279925\n",
      "loss:   0.3626473776996136\n",
      "loss:   0.3501031018793583\n",
      "loss:   0.38707081526517867\n",
      "loss:   0.31960273869335654\n",
      "loss:   0.34272167198359965\n",
      "loss:   0.31818869803100824\n",
      "epoch: 56\n",
      "loss:   0.010960087925195695\n",
      "loss:   0.3446558274328709\n",
      "loss:   0.3710226148366928\n",
      "loss:   0.33987658433616164\n",
      "loss:   0.3601846590638161\n",
      "loss:   0.34192773774266244\n",
      "loss:   0.33526946492493154\n",
      "loss:   0.3471408937126398\n",
      "loss:   0.3537161279469728\n",
      "loss:   0.3531309373676777\n",
      "loss:   0.33557279519736766\n",
      "loss:   0.36006834618747235\n",
      "loss:   0.3554454017430544\n",
      "loss:   0.3315680906176567\n",
      "loss:   0.2999915339052677\n",
      "loss:   0.34699481837451457\n",
      "loss:   0.35638302713632586\n",
      "loss:   0.3312646884471178\n",
      "loss:   0.3558891162276268\n",
      "loss:   0.34903537221252917\n",
      "loss:   0.32309352047741413\n",
      "loss:   0.32900480590760706\n",
      "loss:   0.329224631562829\n",
      "loss:   0.34342284090816977\n",
      "epoch: 57\n",
      "loss:   0.007301890105009079\n",
      "loss:   0.35244260616600515\n",
      "loss:   0.3339445095509291\n",
      "loss:   0.3557958409190178\n",
      "loss:   0.3333877012133598\n",
      "loss:   0.37045006565749644\n",
      "loss:   0.36095663383603094\n",
      "loss:   0.34915669187903403\n",
      "loss:   0.3303052242845297\n",
      "loss:   0.3688290771096945\n",
      "loss:   0.3393263313919306\n",
      "loss:   0.3312986418604851\n",
      "loss:   0.340016820281744\n",
      "loss:   0.3606537666171789\n",
      "loss:   0.34931447021663187\n",
      "loss:   0.31700801122933625\n",
      "loss:   0.33166419304907324\n",
      "loss:   0.34059571400284766\n",
      "loss:   0.3290518283843994\n",
      "loss:   0.3303885653614998\n",
      "loss:   0.3604345589876175\n",
      "loss:   0.33225082568824293\n",
      "loss:   0.32196467407047746\n",
      "loss:   0.30283307023346423\n",
      "epoch: 58\n",
      "loss:   0.009683433920145035\n",
      "loss:   0.3130021158605814\n",
      "loss:   0.3249053444713354\n",
      "loss:   0.33478152863681315\n",
      "loss:   0.3006877895444632\n",
      "loss:   0.36220158338546754\n",
      "loss:   0.3395469758659601\n",
      "loss:   0.32618411630392075\n",
      "loss:   0.360973709449172\n",
      "loss:   0.3473797429352999\n",
      "loss:   0.33544551245868204\n",
      "loss:   0.3420960262417793\n",
      "loss:   0.3492321942001581\n",
      "loss:   0.36572345718741417\n",
      "loss:   0.3304060198366642\n",
      "loss:   0.33619523216038943\n",
      "loss:   0.3570043221116066\n",
      "loss:   0.33374362252652645\n",
      "loss:   0.3046052008867264\n",
      "loss:   0.3542375981807709\n",
      "loss:   0.34036043398082255\n",
      "loss:   0.3381039064377546\n",
      "loss:   0.3454931702464819\n",
      "loss:   0.36635245233774183\n",
      "epoch: 59\n",
      "loss:   0.018266476690769196\n",
      "loss:   0.34453672021627424\n",
      "loss:   0.3450412184000015\n",
      "loss:   0.3507317829877138\n",
      "loss:   0.3096326120197773\n",
      "loss:   0.3652029987424612\n",
      "loss:   0.3214051999151707\n",
      "loss:   0.30672754142433406\n",
      "loss:   0.32949610874056817\n",
      "loss:   0.3451350163668394\n",
      "loss:   0.3417329918593168\n",
      "loss:   0.37218804396688937\n",
      "loss:   0.3459385592490435\n",
      "loss:   0.3323141738772392\n",
      "loss:   0.36645332239568235\n",
      "loss:   0.3237556111067533\n",
      "loss:   0.30326603427529336\n",
      "loss:   0.31157561764121056\n",
      "loss:   0.3489278893917799\n",
      "loss:   0.33777044378221033\n",
      "loss:   0.3137714065611362\n",
      "loss:   0.3492110088467598\n",
      "loss:   0.33139190338552\n",
      "loss:   0.34365214202553035\n",
      "epoch: 60\n",
      "loss:   0.008264050632715226\n",
      "loss:   0.3357371974736452\n",
      "loss:   0.3492338955402374\n",
      "loss:   0.3319497350603342\n",
      "loss:   0.3156159624457359\n",
      "loss:   0.3100645378232002\n",
      "loss:   0.3340906444936991\n",
      "loss:   0.32420981526374815\n",
      "loss:   0.33735122084617614\n",
      "loss:   0.3408317781984806\n",
      "loss:   0.3398725982755423\n",
      "loss:   0.3282146941870451\n",
      "loss:   0.3320355750620365\n",
      "loss:   0.3297486871480942\n",
      "loss:   0.3417081475257874\n",
      "loss:   0.3364694595336914\n",
      "loss:   0.3333490028977394\n",
      "loss:   0.33679359704256057\n",
      "loss:   0.36878793835639956\n",
      "loss:   0.3393918056041002\n",
      "loss:   0.3311190377920866\n",
      "loss:   0.3529504708945751\n",
      "loss:   0.34044601283967496\n",
      "loss:   0.34074153900146487\n",
      "epoch: 61\n",
      "loss:   0.009579699486494064\n",
      "loss:   0.3144127238541842\n",
      "loss:   0.3341785207390785\n",
      "loss:   0.3279163185507059\n",
      "loss:   0.3144148711115122\n",
      "loss:   0.29937429800629617\n",
      "loss:   0.33623340129852297\n",
      "loss:   0.34696683324873445\n",
      "loss:   0.3436571378260851\n",
      "loss:   0.3324038252234459\n",
      "loss:   0.3721894543617964\n",
      "loss:   0.36829972416162493\n",
      "loss:   0.32167573254555465\n",
      "loss:   0.3612607382237911\n",
      "loss:   0.3392941627651453\n",
      "loss:   0.31422562412917615\n",
      "loss:   0.32620260342955587\n",
      "loss:   0.347016691416502\n",
      "loss:   0.3505797512829304\n",
      "loss:   0.3206107094883919\n",
      "loss:   0.31499076820909977\n",
      "loss:   0.36044973209500314\n",
      "loss:   0.3166270021349192\n",
      "loss:   0.32956354804337024\n",
      "epoch: 62\n",
      "loss:   0.006221480295062065\n",
      "loss:   0.34651663191616533\n",
      "loss:   0.34815602414309976\n",
      "loss:   0.33747855238616464\n",
      "loss:   0.37404376119375227\n",
      "loss:   0.3055370349436998\n",
      "loss:   0.36147784255445004\n",
      "loss:   0.31055102944374086\n",
      "loss:   0.30059935711324215\n",
      "loss:   0.3491940300911665\n",
      "loss:   0.3050199054181576\n",
      "loss:   0.332184835895896\n",
      "loss:   0.33286677710711954\n",
      "loss:   0.35226240418851373\n",
      "loss:   0.30036640614271165\n",
      "loss:   0.3303948860615492\n",
      "loss:   0.3396390754729509\n",
      "loss:   0.31077965907752514\n",
      "loss:   0.3401285860687494\n",
      "loss:   0.3239855904132128\n",
      "loss:   0.3047462526708841\n",
      "loss:   0.34610267281532286\n",
      "loss:   0.34396753534674646\n",
      "loss:   0.35011363960802555\n",
      "epoch: 63\n",
      "loss:   0.010606721788644791\n",
      "loss:   0.3428580641746521\n",
      "loss:   0.31575437597930434\n",
      "loss:   0.33494203444570303\n",
      "loss:   0.3333797261118889\n",
      "loss:   0.331441555544734\n",
      "loss:   0.30946997851133345\n",
      "loss:   0.34930968582630156\n",
      "loss:   0.3270917683839798\n",
      "loss:   0.345725629478693\n",
      "loss:   0.34086129926145076\n",
      "loss:   0.3220150750130415\n",
      "loss:   0.33049291446805\n",
      "loss:   0.3219929672777653\n",
      "loss:   0.32465052641928194\n",
      "loss:   0.34184699319303036\n",
      "loss:   0.30936835929751394\n",
      "loss:   0.31733027584850787\n",
      "loss:   0.31194765754044057\n",
      "loss:   0.312345277145505\n",
      "loss:   0.3418124940246344\n",
      "loss:   0.3486842233687639\n",
      "loss:   0.3542124766856432\n",
      "loss:   0.3350849237293005\n",
      "epoch: 64\n",
      "loss:   0.011139687895774842\n",
      "loss:   0.3327073510736227\n",
      "loss:   0.3445818334817886\n",
      "loss:   0.3218384273350239\n",
      "loss:   0.3185568615794182\n",
      "loss:   0.3054599205031991\n",
      "loss:   0.3278422564268112\n",
      "loss:   0.33816217482089994\n",
      "loss:   0.346804129332304\n",
      "loss:   0.31079558059573176\n",
      "loss:   0.2992166727781296\n",
      "loss:   0.33017614111304283\n",
      "loss:   0.34688518904149535\n",
      "loss:   0.3527870949357748\n",
      "loss:   0.33096904270350935\n",
      "loss:   0.31879591643810273\n",
      "loss:   0.3229194778949022\n",
      "loss:   0.3361851923167706\n",
      "loss:   0.34142399951815605\n",
      "loss:   0.3429847113788128\n",
      "loss:   0.3201335959136486\n",
      "loss:   0.337640780210495\n",
      "loss:   0.33502598814666273\n",
      "loss:   0.32021270915865896\n",
      "epoch: 65\n",
      "loss:   0.008449068665504456\n",
      "loss:   0.3307837214320898\n",
      "loss:   0.3380463972687721\n",
      "loss:   0.3263094961643219\n",
      "loss:   0.3073692861944437\n",
      "loss:   0.30104681961238383\n",
      "loss:   0.3320171069353819\n",
      "loss:   0.30661924853920935\n",
      "loss:   0.3454384669661522\n",
      "loss:   0.3016461327672005\n",
      "loss:   0.31729213558137415\n",
      "loss:   0.31301356069743635\n",
      "loss:   0.3302508436143398\n",
      "loss:   0.34222260378301145\n",
      "loss:   0.33879242464900017\n",
      "loss:   0.33960314057767393\n",
      "loss:   0.31700400467962025\n",
      "loss:   0.3308902971446514\n",
      "loss:   0.33165317103266717\n",
      "loss:   0.3128805849701166\n",
      "loss:   0.342484525591135\n",
      "loss:   0.32260271832346915\n",
      "loss:   0.35265208780765533\n",
      "loss:   0.33199160266667604\n",
      "epoch: 66\n",
      "loss:   0.008093910664319992\n",
      "loss:   0.31746508926153183\n",
      "loss:   0.33709555491805077\n",
      "loss:   0.31555183716118335\n",
      "loss:   0.33654584772884844\n",
      "loss:   0.34538877233862875\n",
      "loss:   0.33145200163125993\n",
      "loss:   0.3171622883528471\n",
      "loss:   0.3042629335075617\n",
      "loss:   0.32091807276010514\n",
      "loss:   0.3642175769433379\n",
      "loss:   0.3326810523867607\n",
      "loss:   0.3084947720170021\n",
      "loss:   0.3410492394119501\n",
      "loss:   0.3252390202134848\n",
      "loss:   0.3405078832060099\n",
      "loss:   0.34079760164022443\n",
      "loss:   0.3507058687508106\n",
      "loss:   0.30977944005280733\n",
      "loss:   0.2827666815370321\n",
      "loss:   0.3374703522771597\n",
      "loss:   0.3299842208623886\n",
      "loss:   0.3210771745070815\n",
      "loss:   0.30250959657132626\n",
      "epoch: 67\n",
      "loss:   0.005927749350667\n",
      "loss:   0.326118827983737\n",
      "loss:   0.3342483799904585\n",
      "loss:   0.2966637458652258\n",
      "loss:   0.3128573393449187\n",
      "loss:   0.3520471602678299\n",
      "loss:   0.33685681596398354\n",
      "loss:   0.33669021129608157\n",
      "loss:   0.3096218898892403\n",
      "loss:   0.32549057304859164\n",
      "loss:   0.29201937206089496\n",
      "loss:   0.3418087847530842\n",
      "loss:   0.32774992696940897\n",
      "loss:   0.3148702934384346\n",
      "loss:   0.3173554200679064\n",
      "loss:   0.33439396917819975\n",
      "loss:   0.3020391695201397\n",
      "loss:   0.3110497545450926\n",
      "loss:   0.32921522445976736\n",
      "loss:   0.32643610797822475\n",
      "loss:   0.3281052645295858\n",
      "loss:   0.3840370841324329\n",
      "loss:   0.3102592762559652\n",
      "loss:   0.3083247024565935\n",
      "epoch: 68\n",
      "loss:   0.011880745738744735\n",
      "loss:   0.3458145335316658\n",
      "loss:   0.31130935810506344\n",
      "loss:   0.30734665021300317\n",
      "loss:   0.33652062229812146\n",
      "loss:   0.32949878051877024\n",
      "loss:   0.3303364858031273\n",
      "loss:   0.30965812765061856\n",
      "loss:   0.3265326704829931\n",
      "loss:   0.3177110604941845\n",
      "loss:   0.3292056430131197\n",
      "loss:   0.305707392655313\n",
      "loss:   0.3499239727854729\n",
      "loss:   0.3422221545130014\n",
      "loss:   0.331998023763299\n",
      "loss:   0.31860561668872833\n",
      "loss:   0.31531322561204433\n",
      "loss:   0.28658331781625745\n",
      "loss:   0.3475379802286625\n",
      "loss:   0.31139116995036603\n",
      "loss:   0.3274561882019043\n",
      "loss:   0.3245969720184803\n",
      "loss:   0.3001048345118761\n",
      "loss:   0.3166963405907154\n",
      "epoch: 69\n",
      "loss:   0.005920260399580002\n",
      "loss:   0.3091851774603128\n",
      "loss:   0.3000520208850503\n",
      "loss:   0.3413928925991058\n",
      "loss:   0.29140603393316267\n",
      "loss:   0.3419553622603416\n",
      "loss:   0.3521781638264656\n",
      "loss:   0.2942120272666216\n",
      "loss:   0.3190247505903244\n",
      "loss:   0.31833916530013084\n",
      "loss:   0.30231551323086026\n",
      "loss:   0.30508623719215394\n",
      "loss:   0.3885735973715782\n",
      "loss:   0.33070197366178034\n",
      "loss:   0.3282223291695118\n",
      "loss:   0.33338703624904154\n",
      "loss:   0.3091289035975933\n",
      "loss:   0.3204940542578697\n",
      "loss:   0.3169960444793105\n",
      "loss:   0.31660078912973405\n",
      "loss:   0.3251479484140873\n",
      "loss:   0.335847844183445\n",
      "loss:   0.3265397045761347\n",
      "loss:   0.29485299400985243\n",
      "epoch: 70\n",
      "loss:   0.013266837596893311\n",
      "loss:   0.30155418887734414\n",
      "loss:   0.3196406826376915\n",
      "loss:   0.2955633085221052\n",
      "loss:   0.30021122880280016\n",
      "loss:   0.324649440869689\n",
      "loss:   0.3279179122298956\n",
      "loss:   0.3312216069549322\n",
      "loss:   0.3159740999341011\n",
      "loss:   0.3284465178847313\n",
      "loss:   0.316612683609128\n",
      "loss:   0.3317943211644888\n",
      "loss:   0.3340577306225896\n",
      "loss:   0.31808988451957704\n",
      "loss:   0.3465567950159311\n",
      "loss:   0.31037791073322296\n",
      "loss:   0.30908558834344146\n",
      "loss:   0.317564339376986\n",
      "loss:   0.33040338177233936\n",
      "loss:   0.31482342407107355\n",
      "loss:   0.32465059906244276\n",
      "loss:   0.3329445770010352\n",
      "loss:   0.3002070240676403\n",
      "loss:   0.31336151920259\n",
      "epoch: 71\n",
      "loss:   0.008652985095977783\n",
      "loss:   0.33381106071174144\n",
      "loss:   0.3046910673379898\n",
      "loss:   0.3028432060033083\n",
      "loss:   0.2948183985427022\n",
      "loss:   0.3062284480780363\n",
      "loss:   0.2913947321474552\n",
      "loss:   0.32196575552225115\n",
      "loss:   0.3119791209697723\n",
      "loss:   0.30343094617128374\n",
      "loss:   0.3120446141809225\n",
      "loss:   0.32954579629004\n",
      "loss:   0.35450095124542713\n",
      "loss:   0.33861127533018587\n",
      "loss:   0.3292995039373636\n",
      "loss:   0.32548644915223124\n",
      "loss:   0.29997816067188976\n",
      "loss:   0.32005881890654564\n",
      "loss:   0.3420245125889778\n",
      "loss:   0.3040197597816586\n",
      "loss:   0.30690430775284766\n",
      "loss:   0.30849702209234237\n",
      "loss:   0.32874076440930367\n",
      "loss:   0.3514059100300074\n",
      "epoch: 72\n",
      "loss:   0.009955038875341415\n",
      "loss:   0.2869131240993738\n",
      "loss:   0.33392701875418423\n",
      "loss:   0.35043923147022726\n",
      "loss:   0.3492683496326208\n",
      "loss:   0.31130462475121023\n",
      "loss:   0.3298195838928223\n",
      "loss:   0.33727931454777715\n",
      "loss:   0.31893756091594694\n",
      "loss:   0.3300606433302164\n",
      "loss:   0.3272634711116552\n",
      "loss:   0.3247021403163671\n",
      "loss:   0.30002998895943167\n",
      "loss:   0.3037818044424057\n",
      "loss:   0.30426034331321716\n",
      "loss:   0.3336254138499498\n",
      "loss:   0.3039142079651356\n",
      "loss:   0.27436668463051317\n",
      "loss:   0.32944803312420845\n",
      "loss:   0.3314041391015053\n",
      "loss:   0.31521205846220257\n",
      "loss:   0.32065805941820147\n",
      "loss:   0.3165882624685764\n",
      "loss:   0.2782280322164297\n",
      "epoch: 73\n",
      "loss:   0.010050962120294571\n",
      "loss:   0.30198416151106355\n",
      "loss:   0.29829043596982957\n",
      "loss:   0.30613015554845335\n",
      "loss:   0.3151947408914566\n",
      "loss:   0.322470610961318\n",
      "loss:   0.3121609972789884\n",
      "loss:   0.32510280068963765\n",
      "loss:   0.3017627075314522\n",
      "loss:   0.3225304316729307\n",
      "loss:   0.31384841408580544\n",
      "loss:   0.3225114906206727\n",
      "loss:   0.3271264186128974\n",
      "loss:   0.33744325023144484\n",
      "loss:   0.29143053889274595\n",
      "loss:   0.31580539532005786\n",
      "loss:   0.2980229988694191\n",
      "loss:   0.29322131983935834\n",
      "loss:   0.313919011130929\n",
      "loss:   0.3246163077652454\n",
      "loss:   0.3113208876922727\n",
      "loss:   0.33436487764120104\n",
      "loss:   0.323862723633647\n",
      "loss:   0.342821554094553\n",
      "epoch: 74\n",
      "loss:   0.005598371103405953\n",
      "loss:   0.326451563090086\n",
      "loss:   0.3271229684352875\n",
      "loss:   0.3155053868889809\n",
      "loss:   0.33105471506714823\n",
      "loss:   0.29507916830480097\n",
      "loss:   0.3042045086622238\n",
      "loss:   0.3264722712337971\n",
      "loss:   0.29684898853302\n",
      "loss:   0.30155695118010045\n",
      "loss:   0.3387789785861969\n",
      "loss:   0.2936774805188179\n",
      "loss:   0.3254917846992612\n",
      "loss:   0.3253350082784891\n",
      "loss:   0.30669435784220694\n",
      "loss:   0.32028886713087557\n",
      "loss:   0.29905114844441416\n",
      "loss:   0.3278524599969387\n",
      "loss:   0.30406714528799056\n",
      "loss:   0.30972292087972164\n",
      "loss:   0.31931134797632693\n",
      "loss:   0.2898574136197567\n",
      "loss:   0.3129621665924788\n",
      "loss:   0.3447577480226755\n",
      "epoch: 75\n",
      "loss:   0.003056731820106506\n",
      "loss:   0.33840054292231797\n",
      "loss:   0.32135610580444335\n",
      "loss:   0.30713915955275295\n",
      "loss:   0.28427439257502557\n",
      "loss:   0.31731437183916567\n",
      "loss:   0.2891750831156969\n",
      "loss:   0.2936716940253973\n",
      "loss:   0.3237789086997509\n",
      "loss:   0.3291382536292076\n",
      "loss:   0.2969554657116532\n",
      "loss:   0.3201261982321739\n",
      "loss:   0.30960912853479383\n",
      "loss:   0.3237944021821022\n",
      "loss:   0.29355001635849476\n",
      "loss:   0.3209391823038459\n",
      "loss:   0.3286914134398103\n",
      "loss:   0.30517364144325254\n",
      "loss:   0.2882053032517433\n",
      "loss:   0.3232393074780703\n",
      "loss:   0.3240644734352827\n",
      "loss:   0.309302482008934\n",
      "loss:   0.3084524504840374\n",
      "loss:   0.3315008468925953\n",
      "epoch: 76\n",
      "loss:   0.01162136122584343\n",
      "loss:   0.3375342983752489\n",
      "loss:   0.3392831526696682\n",
      "loss:   0.3131609294563532\n",
      "loss:   0.31912390515208244\n",
      "loss:   0.33404346853494643\n",
      "loss:   0.2906452361494303\n",
      "loss:   0.3182510808110237\n",
      "loss:   0.3076793609187007\n",
      "loss:   0.2873240876942873\n",
      "loss:   0.28903392478823664\n",
      "loss:   0.3198187632486224\n",
      "loss:   0.33101233690977094\n",
      "loss:   0.30104825384914874\n",
      "loss:   0.31391767486929895\n",
      "loss:   0.2803722467273474\n",
      "loss:   0.30310558564960954\n",
      "loss:   0.31147618368268015\n",
      "loss:   0.34019827730953695\n",
      "loss:   0.29186212345957757\n",
      "loss:   0.329435227625072\n",
      "loss:   0.32187600843608377\n",
      "loss:   0.30162755958735943\n",
      "loss:   0.289370684325695\n",
      "epoch: 77\n",
      "loss:   0.009828363358974457\n",
      "loss:   0.30027798302471637\n",
      "loss:   0.2905373336747289\n",
      "loss:   0.3160354094579816\n",
      "loss:   0.3074015200138092\n",
      "loss:   0.3339506294578314\n",
      "loss:   0.30661199651658533\n",
      "loss:   0.31689997799694536\n",
      "loss:   0.30361045226454736\n",
      "loss:   0.3019562367349863\n",
      "loss:   0.3213518522679806\n",
      "loss:   0.30410470701754094\n",
      "loss:   0.32078720927238463\n",
      "loss:   0.3240132562816143\n",
      "loss:   0.31354812942445276\n",
      "loss:   0.2810588013380766\n",
      "loss:   0.3071950193494558\n",
      "loss:   0.2901567626744509\n",
      "loss:   0.31920295022428036\n",
      "loss:   0.30475587472319604\n",
      "loss:   0.32536921054124834\n",
      "loss:   0.2745911104604602\n",
      "loss:   0.3512440785765648\n",
      "loss:   0.30835572592914107\n",
      "epoch: 78\n",
      "loss:   0.005075119435787201\n",
      "loss:   0.29666295163333417\n",
      "loss:   0.31507218535989523\n",
      "loss:   0.31123905796557666\n",
      "loss:   0.3169661661610007\n",
      "loss:   0.3181847333908081\n",
      "loss:   0.3002744508907199\n",
      "loss:   0.31155123449862004\n",
      "loss:   0.286021051555872\n",
      "loss:   0.30911439694464204\n",
      "loss:   0.31373901180922986\n",
      "loss:   0.30021464861929414\n",
      "loss:   0.3142966076731682\n",
      "loss:   0.31433007791638373\n",
      "loss:   0.3103411540389061\n",
      "loss:   0.30922546684741975\n",
      "loss:   0.32601596266031263\n",
      "loss:   0.2990464124828577\n",
      "loss:   0.2859028451144695\n",
      "loss:   0.2997514925897121\n",
      "loss:   0.31507474184036255\n",
      "loss:   0.3319130901247263\n",
      "loss:   0.32725856490433214\n",
      "loss:   0.2955323938280344\n",
      "epoch: 79\n",
      "loss:   0.005670744553208351\n",
      "loss:   0.2839149856939912\n",
      "loss:   0.330830005556345\n",
      "loss:   0.3165392404422164\n",
      "loss:   0.2706422870978713\n",
      "loss:   0.300296763330698\n",
      "loss:   0.311778749525547\n",
      "loss:   0.2999793577939272\n",
      "loss:   0.30113482419401405\n",
      "loss:   0.32896779850125313\n",
      "loss:   0.31146739292889836\n",
      "loss:   0.28990493565797804\n",
      "loss:   0.3116362325847149\n",
      "loss:   0.33825004696846006\n",
      "loss:   0.3054616704583168\n",
      "loss:   0.30153482835739853\n",
      "loss:   0.3236114703118801\n",
      "loss:   0.3177643947303295\n",
      "loss:   0.3007614940404892\n",
      "loss:   0.31218944024294615\n",
      "loss:   0.31316941827535627\n",
      "loss:   0.3217490652576089\n",
      "loss:   0.30143823139369486\n",
      "loss:   0.2830047748982906\n",
      "epoch: 80\n",
      "loss:   0.006444956362247467\n",
      "loss:   0.31199429668486117\n",
      "loss:   0.30332249589264393\n",
      "loss:   0.30837725531309845\n",
      "loss:   0.3357129920274019\n",
      "loss:   0.31063660345971583\n",
      "loss:   0.28899337965995076\n",
      "loss:   0.28453064188361166\n",
      "loss:   0.33097158186137676\n",
      "loss:   0.297131310030818\n",
      "loss:   0.30780868902802466\n",
      "loss:   0.29161835480481385\n",
      "loss:   0.3079755278304219\n",
      "loss:   0.2882240008562803\n",
      "loss:   0.29674609526991846\n",
      "loss:   0.303415222838521\n",
      "loss:   0.3416826333850622\n",
      "loss:   0.29496978111565114\n",
      "loss:   0.32263453397899866\n",
      "loss:   0.2940719913691282\n",
      "loss:   0.2992631135508418\n",
      "loss:   0.3136211259290576\n",
      "loss:   0.29880460556596516\n",
      "loss:   0.30045553147792814\n",
      "epoch: 81\n",
      "loss:   0.007921658456325531\n",
      "loss:   0.32135758809745313\n",
      "loss:   0.3181353107094765\n",
      "loss:   0.28871189057826996\n",
      "loss:   0.28594891503453257\n",
      "loss:   0.31944597270339725\n",
      "loss:   0.31500656735152005\n",
      "loss:   0.28593309335410594\n",
      "loss:   0.2779706668108702\n",
      "loss:   0.3070856902748346\n",
      "loss:   0.3051347639411688\n",
      "loss:   0.31444420479238033\n",
      "loss:   0.2798983195796609\n",
      "loss:   0.33999093621969223\n",
      "loss:   0.29343631900846956\n",
      "loss:   0.3043068692088127\n",
      "loss:   0.2761096525937319\n",
      "loss:   0.3344189032912254\n",
      "loss:   0.3220887020230293\n",
      "loss:   0.29807487353682516\n",
      "loss:   0.2853902019560337\n",
      "loss:   0.3006274584680796\n",
      "loss:   0.3015793712809682\n",
      "loss:   0.32191712539643047\n",
      "epoch: 82\n",
      "loss:   0.009792350232601166\n",
      "loss:   0.3160585731267929\n",
      "loss:   0.30722332354635\n",
      "loss:   0.2909935496747494\n",
      "loss:   0.2949049150571227\n",
      "loss:   0.33724891413003205\n",
      "loss:   0.3033343667164445\n",
      "loss:   0.30893953517079353\n",
      "loss:   0.30697909742593765\n",
      "loss:   0.2820635695010424\n",
      "loss:   0.3077489010989666\n",
      "loss:   0.26886602994054554\n",
      "loss:   0.29780924394726754\n",
      "loss:   0.3068387847393751\n",
      "loss:   0.3056952882558107\n",
      "loss:   0.2883393343538046\n",
      "loss:   0.31868767403066156\n",
      "loss:   0.3103238370269537\n",
      "loss:   0.28891533203423025\n",
      "loss:   0.30132095590233804\n",
      "loss:   0.3118691461160779\n",
      "loss:   0.3004614604637027\n",
      "loss:   0.316813538223505\n",
      "loss:   0.2944785080850124\n",
      "epoch: 83\n",
      "loss:   0.004992055520415306\n",
      "loss:   0.28702398091554643\n",
      "loss:   0.28362220376729963\n",
      "loss:   0.3164217010140419\n",
      "loss:   0.3137146968394518\n",
      "loss:   0.2837755700573325\n",
      "loss:   0.2875432349741459\n",
      "loss:   0.29572270568460224\n",
      "loss:   0.3128937892615795\n",
      "loss:   0.2969631303101778\n",
      "loss:   0.2995057936757803\n",
      "loss:   0.29668079018592836\n",
      "loss:   0.29615613259375095\n",
      "loss:   0.329541264846921\n",
      "loss:   0.30182446613907815\n",
      "loss:   0.31164148449897766\n",
      "loss:   0.3144727278500795\n",
      "loss:   0.2978735687211156\n",
      "loss:   0.3181486580520868\n",
      "loss:   0.28518155943602325\n",
      "loss:   0.29544061571359637\n",
      "loss:   0.3145207103341818\n",
      "loss:   0.2977396650239825\n",
      "loss:   0.30212756916880606\n",
      "epoch: 84\n",
      "loss:   0.004498514160513878\n",
      "loss:   0.30656986236572265\n",
      "loss:   0.3203200500458479\n",
      "loss:   0.3211310859769583\n",
      "loss:   0.31654207482934\n",
      "loss:   0.3031012829393148\n",
      "loss:   0.31799979954957963\n",
      "loss:   0.30544204264879227\n",
      "loss:   0.2887272255495191\n",
      "loss:   0.29882133603096006\n",
      "loss:   0.30184311121702195\n",
      "loss:   0.3049966501072049\n",
      "loss:   0.31127706579864023\n",
      "loss:   0.28999249674379823\n",
      "loss:   0.3102924406528473\n",
      "loss:   0.28739561699330807\n",
      "loss:   0.3149034734815359\n",
      "loss:   0.27050736285746096\n",
      "loss:   0.27061193697154523\n",
      "loss:   0.2876472182571888\n",
      "loss:   0.3003330953419209\n",
      "loss:   0.299882473051548\n",
      "loss:   0.30721069797873496\n",
      "loss:   0.2940388906747103\n",
      "epoch: 85\n",
      "loss:   0.004676564782857895\n",
      "loss:   0.29285742770880463\n",
      "loss:   0.30927296094596385\n",
      "loss:   0.320280971378088\n",
      "loss:   0.31507944539189336\n",
      "loss:   0.2746119059622288\n",
      "loss:   0.29813840277493\n",
      "loss:   0.29388267286121844\n",
      "loss:   0.2802292540669441\n",
      "loss:   0.30934889130294324\n",
      "loss:   0.30709719732403756\n",
      "loss:   0.2929132513701916\n",
      "loss:   0.2941976053640246\n",
      "loss:   0.30905583407729864\n",
      "loss:   0.2931875839829445\n",
      "loss:   0.3167449843138456\n",
      "loss:   0.3223038069903851\n",
      "loss:   0.2707304634153843\n",
      "loss:   0.30747902691364287\n",
      "loss:   0.27950230725109576\n",
      "loss:   0.30580929666757584\n",
      "loss:   0.2965047022327781\n",
      "loss:   0.3022912725806236\n",
      "loss:   0.2816738408058882\n",
      "epoch: 86\n",
      "loss:   0.00461844801902771\n",
      "loss:   0.3331349492073059\n",
      "loss:   0.2954117048531771\n",
      "loss:   0.31696081794798375\n",
      "loss:   0.28952722940593956\n",
      "loss:   0.2608799861744046\n",
      "loss:   0.26927341856062414\n",
      "loss:   0.2947633769363165\n",
      "loss:   0.28669534586369994\n",
      "loss:   0.32508647814393044\n",
      "loss:   0.32287622466683386\n",
      "loss:   0.30050913970917464\n",
      "loss:   0.3022542119026184\n",
      "loss:   0.3007957495748997\n",
      "loss:   0.300921268761158\n",
      "loss:   0.3104876782745123\n",
      "loss:   0.30357937440276145\n",
      "loss:   0.2686697248369455\n",
      "loss:   0.2618210339918733\n",
      "loss:   0.31150981076061723\n",
      "loss:   0.28405057117342947\n",
      "loss:   0.31577320508658885\n",
      "loss:   0.3117308966815472\n",
      "loss:   0.2854246195405722\n",
      "epoch: 87\n",
      "loss:   0.009417257457971572\n",
      "loss:   0.29584138840436935\n",
      "loss:   0.26069541163742543\n",
      "loss:   0.2981657538563013\n",
      "loss:   0.2949795201420784\n",
      "loss:   0.287358707562089\n",
      "loss:   0.2755931533873081\n",
      "loss:   0.3165963601320982\n",
      "loss:   0.28648228086531163\n",
      "loss:   0.33310951553285123\n",
      "loss:   0.29469119422137735\n",
      "loss:   0.30998091399669647\n",
      "loss:   0.27460633106529714\n",
      "loss:   0.29535074159502983\n",
      "loss:   0.29880045875906946\n",
      "loss:   0.3189924467355013\n",
      "loss:   0.29971805699169635\n",
      "loss:   0.3098643448203802\n",
      "loss:   0.32184602580964566\n",
      "loss:   0.289463634416461\n",
      "loss:   0.2884418532252312\n",
      "loss:   0.2778043545782566\n",
      "loss:   0.3144962254911661\n",
      "loss:   0.28818969167768954\n",
      "epoch: 88\n",
      "loss:   0.006398147344589234\n",
      "loss:   0.2780588693916798\n",
      "loss:   0.30262196958065035\n",
      "loss:   0.30979937631636856\n",
      "loss:   0.294841893017292\n",
      "loss:   0.3025104381144047\n",
      "loss:   0.3005121374502778\n",
      "loss:   0.2911490464583039\n",
      "loss:   0.2842535961419344\n",
      "loss:   0.31166706532239913\n",
      "loss:   0.2909845069050789\n",
      "loss:   0.277485060878098\n",
      "loss:   0.3009656170383096\n",
      "loss:   0.28236254304647446\n",
      "loss:   0.3073405236005783\n",
      "loss:   0.3160550381988287\n",
      "loss:   0.297898418083787\n",
      "loss:   0.28025746569037435\n",
      "loss:   0.297232973203063\n",
      "loss:   0.2891679421067238\n",
      "loss:   0.32431613057851794\n",
      "loss:   0.2918365398421884\n",
      "loss:   0.29210143610835076\n",
      "loss:   0.2959462359547615\n",
      "epoch: 89\n",
      "loss:   0.010773590952157974\n",
      "loss:   0.3021050330251455\n",
      "loss:   0.3179793544113636\n",
      "loss:   0.28103259298950434\n",
      "loss:   0.29195995144546033\n",
      "loss:   0.2954360041767359\n",
      "loss:   0.2933708693832159\n",
      "loss:   0.31484988555312154\n",
      "loss:   0.2751545386388898\n",
      "loss:   0.2997178893536329\n",
      "loss:   0.2833707958459854\n",
      "loss:   0.2971059739589691\n",
      "loss:   0.312972978502512\n",
      "loss:   0.2967194255441427\n",
      "loss:   0.28134522400796413\n",
      "loss:   0.2900967814028263\n",
      "loss:   0.3087119575589895\n",
      "loss:   0.2966399073600769\n",
      "loss:   0.27140368036925794\n",
      "loss:   0.2824901919811964\n",
      "loss:   0.3183298874646425\n",
      "loss:   0.26163571029901506\n",
      "loss:   0.3141928756609559\n",
      "loss:   0.2906397543847561\n",
      "epoch: 90\n",
      "loss:   0.010718327760696412\n",
      "loss:   0.28619978595525025\n",
      "loss:   0.29307134244591\n",
      "loss:   0.29027293622493744\n",
      "loss:   0.2974713008850813\n",
      "loss:   0.30457007195800545\n",
      "loss:   0.32099145613610747\n",
      "loss:   0.2855722503736615\n",
      "loss:   0.265270291082561\n",
      "loss:   0.26814782023429873\n",
      "loss:   0.2942184492945671\n",
      "loss:   0.3018527217209339\n",
      "loss:   0.3059972867369652\n",
      "loss:   0.289618705958128\n",
      "loss:   0.2848126754164696\n",
      "loss:   0.27887798361480237\n",
      "loss:   0.27898474261164663\n",
      "loss:   0.33268570601940156\n",
      "loss:   0.2848921179771423\n",
      "loss:   0.29352696482092144\n",
      "loss:   0.29367745816707613\n",
      "loss:   0.27923053205013276\n",
      "loss:   0.3050295524299145\n",
      "loss:   0.29897128865122796\n",
      "epoch: 91\n",
      "loss:   0.008245178312063218\n",
      "loss:   0.2688200891017914\n",
      "loss:   0.2684827815741301\n",
      "loss:   0.29695669934153557\n",
      "loss:   0.3126171052455902\n",
      "loss:   0.2930129963904619\n",
      "loss:   0.29068989492952824\n",
      "loss:   0.279635439068079\n",
      "loss:   0.2974272042512894\n",
      "loss:   0.28363779094070196\n",
      "loss:   0.28783459924161436\n",
      "loss:   0.30119894035160544\n",
      "loss:   0.2713619787245989\n",
      "loss:   0.29450027346611024\n",
      "loss:   0.3103314207866788\n",
      "loss:   0.28761707041412593\n",
      "loss:   0.2883189953863621\n",
      "loss:   0.3213569361716509\n",
      "loss:   0.29458515215665104\n",
      "loss:   0.29169881381094453\n",
      "loss:   0.29641397558152677\n",
      "loss:   0.2828407995402813\n",
      "loss:   0.2806797964498401\n",
      "loss:   0.3181756973266602\n",
      "epoch: 92\n",
      "loss:   0.009168615937232972\n",
      "loss:   0.29018592108041047\n",
      "loss:   0.3080891838297248\n",
      "loss:   0.2755981061607599\n",
      "loss:   0.25595253743231294\n",
      "loss:   0.28859097566455605\n",
      "loss:   0.27813784386962653\n",
      "loss:   0.2845425151288509\n",
      "loss:   0.31875754818320273\n",
      "loss:   0.2601716458797455\n",
      "loss:   0.31456117164343594\n",
      "loss:   0.29184908214956523\n",
      "loss:   0.2879909908398986\n",
      "loss:   0.29275986049324276\n",
      "loss:   0.2939932826906443\n",
      "loss:   0.2879125425592065\n",
      "loss:   0.3019417837262154\n",
      "loss:   0.27967722304165366\n",
      "loss:   0.29680209551006553\n",
      "loss:   0.30779243241995574\n",
      "loss:   0.27085488066077235\n",
      "loss:   0.2964137554168701\n",
      "loss:   0.2883176181465387\n",
      "loss:   0.3133691899478436\n",
      "epoch: 93\n",
      "loss:   0.006388264149427414\n",
      "loss:   0.31215282082557677\n",
      "loss:   0.2656748365610838\n",
      "loss:   0.291827654838562\n",
      "loss:   0.31379069685935973\n",
      "loss:   0.27143306750804186\n",
      "loss:   0.30852405428886415\n",
      "loss:   0.28116677384823563\n",
      "loss:   0.24865103494375945\n",
      "loss:   0.2763853408396244\n",
      "loss:   0.2983898339793086\n",
      "loss:   0.291874647885561\n",
      "loss:   0.2917168099433184\n",
      "loss:   0.2964687116444111\n",
      "loss:   0.27244487274438145\n",
      "loss:   0.3198974747210741\n",
      "loss:   0.285078888759017\n",
      "loss:   0.3009622924029827\n",
      "loss:   0.30597347151488063\n",
      "loss:   0.28457415755838156\n",
      "loss:   0.2706568919122219\n",
      "loss:   0.3035441394895315\n",
      "loss:   0.2894934467971325\n",
      "loss:   0.2775259234011173\n",
      "epoch: 94\n",
      "loss:   0.006251086294651031\n",
      "loss:   0.29701755587011575\n",
      "loss:   0.2622558631002903\n",
      "loss:   0.3055620525032282\n",
      "loss:   0.30015632435679435\n",
      "loss:   0.3074696116149426\n",
      "loss:   0.290039087831974\n",
      "loss:   0.3000307846814394\n",
      "loss:   0.2610824516043067\n",
      "loss:   0.28394358344376086\n",
      "loss:   0.2911608463153243\n",
      "loss:   0.2733969550579786\n",
      "loss:   0.28727901428937913\n",
      "loss:   0.3055525653064251\n",
      "loss:   0.2809414634481072\n",
      "loss:   0.2790983906015754\n",
      "loss:   0.29114116840064524\n",
      "loss:   0.2859053125604987\n",
      "loss:   0.3035314017906785\n",
      "loss:   0.30252279676496985\n",
      "loss:   0.2916928015649319\n",
      "loss:   0.2910196755081415\n",
      "loss:   0.2694806994870305\n",
      "loss:   0.2757133308798075\n",
      "epoch: 95\n",
      "loss:   0.00398896150290966\n",
      "loss:   0.3126777369529009\n",
      "loss:   0.28372797928750515\n",
      "loss:   0.2836966933682561\n",
      "loss:   0.2700152963399887\n",
      "loss:   0.27948308661580085\n",
      "loss:   0.2995831560343504\n",
      "loss:   0.26772269010543825\n",
      "loss:   0.27015522196888925\n",
      "loss:   0.2941513884812593\n",
      "loss:   0.29755355417728424\n",
      "loss:   0.3063995081931353\n",
      "loss:   0.2867257446050644\n",
      "loss:   0.2749415995553136\n",
      "loss:   0.29399765469133854\n",
      "loss:   0.30435075517743826\n",
      "loss:   0.286910368129611\n",
      "loss:   0.3010154590010643\n",
      "loss:   0.2944287031888962\n",
      "loss:   0.2882222138345242\n",
      "loss:   0.28236954770982264\n",
      "loss:   0.27489820644259455\n",
      "loss:   0.2691102704033256\n",
      "loss:   0.29226402938365936\n",
      "epoch: 96\n",
      "loss:   0.008524985611438751\n",
      "loss:   0.2868754068389535\n",
      "loss:   0.2930349938571453\n",
      "loss:   0.31997706592082975\n",
      "loss:   0.27173196747899053\n",
      "loss:   0.32149538919329645\n",
      "loss:   0.28712940951809285\n",
      "loss:   0.2837133167311549\n",
      "loss:   0.3074956638738513\n",
      "loss:   0.29382572770118714\n",
      "loss:   0.29283856227993965\n",
      "loss:   0.28499803356826303\n",
      "loss:   0.25799825731664894\n",
      "loss:   0.25060777701437476\n",
      "loss:   0.2812474872916937\n",
      "loss:   0.26757278516888616\n",
      "loss:   0.29307754151523113\n",
      "loss:   0.2733888188377023\n",
      "loss:   0.30232611745595933\n",
      "loss:   0.27497807424515486\n",
      "loss:   0.28732292987406255\n",
      "loss:   0.29423708505928514\n",
      "loss:   0.2787866972386837\n",
      "loss:   0.2841441806405783\n",
      "epoch: 97\n",
      "loss:   0.008231403678655625\n",
      "loss:   0.3054611383005977\n",
      "loss:   0.24923163875937462\n",
      "loss:   0.2622503239661455\n",
      "loss:   0.2823489461094141\n",
      "loss:   0.28396328166127205\n",
      "loss:   0.30303072929382324\n",
      "loss:   0.2732028605416417\n",
      "loss:   0.2643236370757222\n",
      "loss:   0.30718942917883396\n",
      "loss:   0.29227243438363076\n",
      "loss:   0.28687852527946234\n",
      "loss:   0.25565170105546714\n",
      "loss:   0.28840332105755806\n",
      "loss:   0.2947313152253628\n",
      "loss:   0.28425650373101236\n",
      "loss:   0.258877651207149\n",
      "loss:   0.31364473402500154\n",
      "loss:   0.296279775723815\n",
      "loss:   0.2816001920029521\n",
      "loss:   0.298109682649374\n",
      "loss:   0.26857613027095795\n",
      "loss:   0.29145593773573636\n",
      "loss:   0.2957464290782809\n",
      "epoch: 98\n",
      "loss:   0.008214324712753296\n",
      "loss:   0.28374905325472355\n",
      "loss:   0.28213745169341564\n",
      "loss:   0.27255993485450747\n",
      "loss:   0.2773905986919999\n",
      "loss:   0.2709424287080765\n",
      "loss:   0.29391161873936655\n",
      "loss:   0.2995245035737753\n",
      "loss:   0.2942609993740916\n",
      "loss:   0.27267036512494086\n",
      "loss:   0.2843568714335561\n",
      "loss:   0.2982037734240294\n",
      "loss:   0.3014421686530113\n",
      "loss:   0.2817287685349584\n",
      "loss:   0.26884270142763855\n",
      "loss:   0.27702610902488234\n",
      "loss:   0.2737778428941965\n",
      "loss:   0.2902140311896801\n",
      "loss:   0.2930543128401041\n",
      "loss:   0.2992490757256746\n",
      "loss:   0.25596080143004657\n",
      "loss:   0.29071855545043945\n",
      "loss:   0.2538458824157715\n",
      "loss:   0.306642877869308\n",
      "epoch: 99\n",
      "loss:   0.0074559181928634645\n",
      "loss:   0.30144740603864195\n",
      "loss:   0.27572319768369197\n",
      "loss:   0.30757707115262745\n",
      "loss:   0.3029049439355731\n",
      "loss:   0.2963380806148052\n",
      "loss:   0.24031639471650124\n",
      "loss:   0.2625429093837738\n",
      "loss:   0.2767682509496808\n",
      "loss:   0.2740479500964284\n",
      "loss:   0.24720369279384613\n",
      "loss:   0.29790855292230844\n",
      "loss:   0.3052950507029891\n",
      "loss:   0.27554300930351017\n",
      "loss:   0.2891993444412947\n",
      "loss:   0.2855248272418976\n",
      "loss:   0.2768226932734251\n",
      "loss:   0.272080359980464\n",
      "loss:   0.28144003357738256\n",
      "loss:   0.27690667361021043\n",
      "loss:   0.29850405529141427\n",
      "loss:   0.2784071572124958\n",
      "loss:   0.2715780595317483\n",
      "loss:   0.29784284979104997\n"
     ]
    }
   ],
   "source": [
    "## TODO: Your training loop here\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0003)\n",
    "\n",
    "epochs = 100\n",
    "print_every = 40\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    print(f\"epoch: {e}\")\n",
    "\n",
    "    for i , (images, labels) in enumerate(iter(trainloader)):\n",
    "\n",
    "        images.resize_(images.size()[0], 784)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model.forward(images)\n",
    "        loss = criterion(pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print(f\"loss:   {running_loss/print_every}\")\n",
    "            running_loss = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9p/tr5nxpls61zfnl9tjr25j0_40000gn/T/ipykernel_22469/3025948180.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  ps = F.softmax(logits)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 195,
       "width": 349
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell with your model to make sure it works and predicts well for the validation data\n",
    "images, labels = next(iter(testloader))\n",
    "images.resize_(images.shape[0], 1, 784)\n",
    "logits = model.forward(images[0,:])\n",
    "ps = F.softmax(logits)\n",
    "view_classify(images[0].view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#222222; color:#ffffff; padding:20px\">\n",
    "  <h3 style=\"color:#01ff84; margin-top:4px\">Exercise 3:</h3>\n",
    "  <p>Write the code for adding <strong style=\"color:#01ff84\">Early Stopping with patience = 2</strong> to the training loop from scratch.</p>\n",
    "  <p><strong style=\"color:#01ff84\">Hint:</strong> Monitor the Validation loss every epoch, and if in 2 epochs, the validation loss does not improve, stop the training loop with <code>break</code>.</p>\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Your training loop here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
